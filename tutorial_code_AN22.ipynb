{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"tutorial_code_AN22.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# [MS65] Tutorials for Students: An Introduction to Sampling with Measure Transport"],"metadata":{"id":"4XFhZMWrD3kC"}},{"cell_type":"markdown","source":["## Software installation (~5min)"],"metadata":{"id":"b64U1f9cETls"}},{"cell_type":"markdown","source":["Installation via git repo:"],"metadata":{"id":"pHgpvW3UHtgH"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"862Pty9lD0jB","outputId":"482e4839-9b8c-410c-98ac-d44293fa3ebb","executionInfo":{"status":"ok","timestamp":1657424214257,"user_tz":240,"elapsed":19904,"user":{"displayName":"Paul-Baptiste RUBIO","userId":"15146079832390040200"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n","Cloning into 'MParT'...\n","remote: Enumerating objects: 4769, done.\u001b[K\n","remote: Counting objects: 100% (609/609), done.\u001b[K\n","remote: Compressing objects: 100% (314/314), done.\u001b[K\n","remote: Total 4769 (delta 306), reused 540 (delta 289), pack-reused 4160\u001b[K\n","Receiving objects: 100% (4769/4769), 5.88 MiB | 10.29 MiB/s, done.\n","Resolving deltas: 100% (2896/2896), done.\n","Get:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n","Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n","Get:3 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n","Hit:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease\n","Ign:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n","Hit:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n","Hit:7 http://archive.ubuntu.com/ubuntu bionic InRelease\n","Get:8 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n","Hit:9 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n","Hit:10 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n","Get:11 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n","Hit:12 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n","Get:13 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,871 kB]\n","Get:14 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,521 kB]\n","Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [3,302 kB]\n","Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,298 kB]\n","Fetched 10.2 MB in 8s (1,232 kB/s)\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","63 packages can be upgraded. Run 'apt list --upgradable' to see them.\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","cmake is already the newest version (3.10.2-1ubuntu2.18.04.2).\n","The following package was automatically installed and is no longer required:\n","  libnvidia-common-460\n","Use 'apt autoremove' to remove it.\n","0 upgraded, 0 newly installed, 0 to remove and 63 not upgraded.\n"]}],"source":["# Git clone\n","# Verify how this work with key and other users\n","# Make repo, public?\n","# Use a minimal version branch? (without tests, other bindings etc.)\n","# Use conda?\n","\n","%cd /content/\n","#!git clone https://rubiop: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@github.com/MeasureTransport/MParT.git\n","\n","!git clone --branch ncpus-python https://rubiop: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@github.com/MeasureTransport/MParT.git\n","\n","\n","#CMake install\n","!apt update\n","!apt install -y cmake"]},{"cell_type":"code","source":["#Go to build folder\n","%cd /content/MParT/\n","!mkdir build\n","%cd build\n","\n","#Run cmake\n","!cmake -DCMAKE_INSTALL_PREFIX=/content/Installations/MParT -DPYTHON_EXECUTABLE=`which python` -DKokkos_ENABLE_PTHREAD=ON -DKokkos_ENABLE_SERIAL=ON ..\n","!make install\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MZzm6DwWPCbL","outputId":"576ebe7a-12e0-4468-cdda-5384fb393c3a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/MParT\n","/content/MParT/build\n","-- The C compiler identification is GNU 7.5.0\n","-- The CXX compiler identification is GNU 7.5.0\n","-- Detecting C compiler ABI info\n","-- Detecting C compiler ABI info - done\n","-- Check for working C compiler: /usr/bin/cc - skipped\n","-- Detecting C compile features\n","-- Detecting C compile features - done\n","-- Detecting CXX compiler ABI info\n","-- Detecting CXX compiler ABI info - done\n","-- Check for working CXX compiler: /usr/bin/c++ - skipped\n","-- Detecting CXX compile features\n","-- Detecting CXX compile features - done\n","-- Performing Test COMPILER_IS_NVCC1\n","-- Performing Test COMPILER_IS_NVCC1 - Success\n","-- Performing Test COMPILER_IS_NVCC2\n","-- Performing Test COMPILER_IS_NVCC2 - Failed\n","-- Could not find Kokkos.  Using internal build.\n","-- Updating GIT Submodule kokkos\n","-- Submodule update\n","Submodule 'external/kokkos' (https://github.com/kokkos/kokkos.git) registered for path '../external/kokkos'\n","Cloning into '/content/MParT/external/kokkos'...\n","Submodule path '../external/kokkos': checked out '2834f94af9b01debf67c1aaa3f0eb0c903d72c8d'\n","-- Setting default Kokkos CXX standard to 17\n","-- Setting policy CMP0074 to use <Package>_ROOT variables\n","-- The project name is: Kokkos\n","-- Using -std=gnu++1z for C++17 extensions as feature\n","-- Built-in Execution Spaces:\n","--     Device Parallel: NoTypeDefined\n","--     Host Parallel: Kokkos::Threads\n","--       Host Serial: SERIAL\n","-- \n","-- Architectures:\n","-- Found TPLLIBDL: /usr/lib/x86_64-linux-gnu/libdl.so  \n","-- Found TPLPTHREAD: TRUE  \n","-- Kokkos Devices: PTHREAD;SERIAL, Kokkos Backends: THREADS;SERIAL\n","-- Could not find Eigen.  Using internal build.\n","-- Updating GIT Submodule eigen\n","-- Submodule update\n","Submodule 'external/eigen' (https://gitlab.com/libeigen/eigen.git) registered for path '../external/eigen'\n","Cloning into '/content/MParT/external/eigen'...\n","Submodule path '../external/eigen': checked out '8d81a2339cde86ff21aec665ed07bb47bb632182'\n","-- Performing Test standard_math_library_linked_to_automatically\n","-- Performing Test standard_math_library_linked_to_automatically - Success\n","-- Standard libraries to link to explicitly: none\n","-- Performing Test COMPILER_SUPPORT_WERROR\n","-- Performing Test COMPILER_SUPPORT_WERROR - Success\n","-- Performing Test COMPILER_SUPPORT_pedantic\n","-- Performing Test COMPILER_SUPPORT_pedantic - Success\n","-- Performing Test COMPILER_SUPPORT_Wall\n","-- Performing Test COMPILER_SUPPORT_Wall - Success\n","-- Performing Test COMPILER_SUPPORT_Wextra\n","-- Performing Test COMPILER_SUPPORT_Wextra - Success\n","-- Performing Test COMPILER_SUPPORT_Wundef\n","-- Performing Test COMPILER_SUPPORT_Wundef - Success\n","-- Performing Test COMPILER_SUPPORT_Wcastalign\n","-- Performing Test COMPILER_SUPPORT_Wcastalign - Success\n","-- Performing Test COMPILER_SUPPORT_Wcharsubscripts\n","-- Performing Test COMPILER_SUPPORT_Wcharsubscripts - Success\n","-- Performing Test COMPILER_SUPPORT_Wnonvirtualdtor\n","-- Performing Test COMPILER_SUPPORT_Wnonvirtualdtor - Success\n","-- Performing Test COMPILER_SUPPORT_Wunusedlocaltypedefs\n","-- Performing Test COMPILER_SUPPORT_Wunusedlocaltypedefs - Success\n","-- Performing Test COMPILER_SUPPORT_Wpointerarith\n","-- Performing Test COMPILER_SUPPORT_Wpointerarith - Success\n","-- Performing Test COMPILER_SUPPORT_Wwritestrings\n","-- Performing Test COMPILER_SUPPORT_Wwritestrings - Success\n","-- Performing Test COMPILER_SUPPORT_Wformatsecurity\n","-- Performing Test COMPILER_SUPPORT_Wformatsecurity - Success\n","-- Performing Test COMPILER_SUPPORT_Wshorten64to32\n","-- Performing Test COMPILER_SUPPORT_Wshorten64to32 - Failed\n","-- Performing Test COMPILER_SUPPORT_Wlogicalop\n","-- Performing Test COMPILER_SUPPORT_Wlogicalop - Success\n","-- Performing Test COMPILER_SUPPORT_Wenumconversion\n","-- Performing Test COMPILER_SUPPORT_Wenumconversion - Failed\n","-- Performing Test COMPILER_SUPPORT_Wcpp11extensions\n","-- Performing Test COMPILER_SUPPORT_Wcpp11extensions - Failed\n","-- Performing Test COMPILER_SUPPORT_Wdoublepromotion\n","-- Performing Test COMPILER_SUPPORT_Wdoublepromotion - Success\n","-- Performing Test COMPILER_SUPPORT_Wshadow\n","-- Performing Test COMPILER_SUPPORT_Wshadow - Success\n","-- Performing Test COMPILER_SUPPORT_Wnopsabi\n","-- Performing Test COMPILER_SUPPORT_Wnopsabi - Success\n","-- Performing Test COMPILER_SUPPORT_Wnovariadicmacros\n","-- Performing Test COMPILER_SUPPORT_Wnovariadicmacros - Success\n","-- Performing Test COMPILER_SUPPORT_Wnolonglong\n","-- Performing Test COMPILER_SUPPORT_Wnolonglong - Success\n","-- Performing Test COMPILER_SUPPORT_fnochecknew\n","-- Performing Test COMPILER_SUPPORT_fnochecknew - Success\n","-- Performing Test COMPILER_SUPPORT_fnocommon\n","-- Performing Test COMPILER_SUPPORT_fnocommon - Success\n","-- Performing Test COMPILER_SUPPORT_fstrictaliasing\n","-- Performing Test COMPILER_SUPPORT_fstrictaliasing - Success\n","-- Performing Test COMPILER_SUPPORT_wd981\n","-- Performing Test COMPILER_SUPPORT_wd981 - Failed\n","-- Performing Test COMPILER_SUPPORT_wd2304\n","-- Performing Test COMPILER_SUPPORT_wd2304 - Failed\n","-- Performing Test COMPILER_SUPPORT_OPENMP\n","-- Performing Test COMPILER_SUPPORT_OPENMP - Success\n","-- Found unsuitable Qt version \"5.9.5\" from /usr/bin/qmake\n","-- Looking for a Fortran compiler\n","-- Looking for a Fortran compiler - /usr/bin/f95\n","-- The Fortran compiler identification is GNU 7.5.0\n","-- Detecting Fortran compiler ABI info\n","-- Detecting Fortran compiler ABI info - done\n","-- Check for working Fortran compiler: /usr/bin/f95 - skipped\n","-- Found unsuitable Qt version \"5.9.5\" from /usr/bin/qmake\n","-- Qt4 not found, so disabling the mandelbrot and opengl demos\n","-- Could NOT find CLANG_FORMAT: Found unsuitable version \"0.0\", but required is exact version \"9\" (found CLANG_FORMAT_EXECUTABLE-NOTFOUND)\n","-- Could NOT find CHOLMOD (missing: CHOLMOD_INCLUDES CHOLMOD_LIBRARIES) \n","-- Could NOT find UMFPACK (missing: UMFPACK_INCLUDES UMFPACK_LIBRARIES) \n","-- Could NOT find KLU (missing: KLU_INCLUDES KLU_LIBRARIES) \n","-- Performing Test SUPERLU_HAS_GLOBAL_MEM_USAGE_T\n","-- Performing Test SUPERLU_HAS_GLOBAL_MEM_USAGE_T - Success\n","-- Performing Test SUPERLU_HAS_CLEAN_ENUMS\n","-- Performing Test SUPERLU_HAS_CLEAN_ENUMS - Success\n","-- Performing Test SUPERLU_HAS_GLOBALLU_T\n","-- Performing Test SUPERLU_HAS_GLOBALLU_T - Success\n","-- Found SuperLU: /usr/include/superlu (found suitable version \"5.0\", minimum required is \"4.0\") \n","-- Checking for one of the modules 'hwloc'\n","-- Performing Test HAVE_HWLOC_PARENT_MEMBER\n","-- Performing Test HAVE_HWLOC_PARENT_MEMBER - Success\n","-- Performing Test HAVE_HWLOC_CACHE_ATTR\n","-- Performing Test HAVE_HWLOC_CACHE_ATTR - Success\n","-- Performing Test HAVE_HWLOC_OBJ_PU\n","-- Performing Test HAVE_HWLOC_OBJ_PU - Success\n","-- Looking for hwloc_bitmap_free in hwloc\n","-- Looking for hwloc_bitmap_free in hwloc - found\n","-- A version of Pastix has been found but pastix_nompi.h does not exist in the include directory. Because Eigen tests require a version without MPI, we disable the Pastix backend.\n","-- \n","-- Configured Eigen 3.4.90\n","-- \n","-- Could not find pybind11. Using internal build.\n","-- Updating GIT Submodule pybind11\n","-- Submodule update\n","Submodule 'external/pybind11' (https://github.com/pybind/pybind11.git) registered for path '../external/pybind11'\n","Cloning into '/content/MParT/external/pybind11'...\n","Submodule path '../external/pybind11': checked out 'ffa346860b306c9bbfb341aed9c14c067751feb8'\n","-- pybind11 v2.9.1 \n","-- Found PythonInterp: /usr/local/bin/python (found version \"3.7.13\") \n","-- Found PythonLibs: /usr/lib/x86_64-linux-gnu/libpython3.7m.so\n","-- Performing Test HAS_FLTO\n","-- Performing Test HAS_FLTO - Success\n","-- Could not find Catch2.  Using internal build.\n","-- Updating GIT Submodule catch2\n","-- Submodule update\n","Submodule 'external/catch2' (https://github.com/catchorg/Catch2) registered for path '../external/catch2'\n","Cloning into '/content/MParT/external/catch2'...\n","Submodule path '../external/catch2': checked out 'b9853b4b356b83bb580c746c3a1f11101f9af54f'\n","-- Pinning GIT Submodule catch2 to version v3.0.0-preview3\n","-- Submodule update\n","HEAD is now at b9853b4b Bump version to v3.0.0 preview 3\n","-- Could NOT find Doxygen (missing: DOXYGEN_EXECUTABLE) \n","-- Found Sphinx: /usr/local/bin/sphinx-build  \n","-- Configuring done\n","-- Generating done\n","-- Build files have been written to: /content/MParT/build\n","[  0%] \u001b[32mBuilding CXX object external/kokkos/core/src/CMakeFiles/kokkoscore.dir/impl/Kokkos_CPUDiscovery.cpp.o\u001b[0m\n","[  0%] \u001b[32mBuilding CXX object external/kokkos/core/src/CMakeFiles/kokkoscore.dir/impl/Kokkos_Core.cpp.o\u001b[0m\n","[  0%] \u001b[32mBuilding CXX object external/kokkos/core/src/CMakeFiles/kokkoscore.dir/impl/Kokkos_Error.cpp.o\u001b[0m\n","[  0%] \u001b[32mBuilding CXX object external/kokkos/core/src/CMakeFiles/kokkoscore.dir/impl/Kokkos_ExecPolicy.cpp.o\u001b[0m\n","[  8%] \u001b[32mBuilding CXX object external/kokkos/core/src/CMakeFiles/kokkoscore.dir/impl/Kokkos_HostBarrier.cpp.o\u001b[0m\n","[  8%] \u001b[32mBuilding CXX object external/kokkos/core/src/CMakeFiles/kokkoscore.dir/impl/Kokkos_HostSpace.cpp.o\u001b[0m\n"]}]},{"cell_type":"code","source":["#Copy librairy to lib folder\n","!cp -r /content/Installations/MParT/lib/* /usr/lib/\n","\n","#Go out to build folder\n","import sys\n","import os\n","sys.path.insert(1, \"/content/Installations/MParT/python/\")\n","%cd /"],"metadata":{"id":"ICgu0xwnb2hM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Import useful python packages:"],"metadata":{"id":"Ff0WEclMGKPv"}},{"cell_type":"code","source":["from mpart import *\n","import numpy as np\n","from scipy.optimize import minimize\n","from scipy.stats import norm\n","import matplotlib.pyplot as plt\n","import scipy.stats\n","import copy\n","from scipy.stats import multivariate_normal\n","import time\n","\n","plt.rcParams['figure.dpi'] = 120\n","plt.rcParams['savefig.dpi'] = 300"],"metadata":{"id":"JF00-Y5rFbpV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## The transport map object: a parametrized class of monotone functions"],"metadata":{"id":"NNQGXtzSfzV4"}},{"cell_type":"markdown","source":["The first property of triangular transport maps that we can highlight is that each component is monotone with respect to the last variable. In order to parametrized the class of monotone functions we use the concept of rectifier (see https://arxiv.org/pdf/2009.10303.pdf). The objective of the rectifier is to transform any multivariate functions to a monotone function with respect to the last variable. The rectifier $R(f)$ of a function $f:\\mathbb{R}^d → \\mathbb{R}^d$ is defined as follow:\n","\\begin{equation}\n","R(f)(x_{1:d}) = f(x_1,...,x_{d-1},0) + \\int_0^{x_d} g\\left(\\partial_{x_d} f(x_1,...,x_{d-1},t\\right) \\text{d}t\n","\\end{equation}\n","where $g$ is a real positive function e.g.:\n","\\begin{equation}\n","g(\\xi) = \\frac{\\log(1+2^\\xi)}{\\log(2)}\n","\\end{equation}"],"metadata":{"id":"9uEWGMfc2IpL"}},{"cell_type":"markdown","source":["### A collection of polynomials and their corresponding monotone functions in 1D:"],"metadata":{"id":"fDfJf5Bo4v8M"}},{"cell_type":"code","source":["n = 100\n","x = np.linspace(-2,2,n)\n","x = x.reshape(1,n)\n","\n","opts = MapOptions()\n","\n","\n","fig,ax = plt.subplots(1,2)\n","for k in range(8):\n","  map_order = np.random.randint(1,6)\n","  map_1d = CreateTriangular(1,1,map_order,opts)\n","  coeffs = 0.5*np.random.randn(map_1d.numCoeffs)\n","  map_1d.SetCoeffs(coeffs) \n","  f1 = map_1d.GetComponent(0).GetBaseFunction()\n","  f1.SetCoeffs(coeffs)\n","  Sy = map_1d.Evaluate(x)\n","  y = f1.Evaluate(x)\n","  ax[0].plot(x.flatten(),y.flatten())\n","  ax[0].set_title('Polynomials')\n","  ax[1].plot(x.flatten(),Sy.flatten())\n","  ax[1].set_title('Monotone functions')\n","  del(f1)\n","plt.show()"],"metadata":{"id":"yDdbu2l2lLmL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Use monotone parametrization for regression \n","\n","One direct use of this property given by map parametrization is to model monotone function from noisy data. This field also called isotonic regression can be solved via minimization of the following objective function:"],"metadata":{"id":"HlAy_J9Of7bM"}},{"cell_type":"markdown","source":["\\begin{equation}\n","J(\\mathbf{w})= \\frac{1}{2} \\sum_{i=1}^N \\left(S(x^i;\\mathbf{w}) - y^i \\right)^2\n","\\end{equation}"],"metadata":{"id":"oKTfJ7iJjV1f"}},{"cell_type":"markdown","source":["where is a monotone 1D map with parameters (polynomial coefficients) $\\mathbf{w}$ and $y^i$ are noisy observations."],"metadata":{"id":"paZJUHgHoQp0"}},{"cell_type":"markdown","source":["The corresponding gradient objective reads:"],"metadata":{"id":"Udc1yT_noqKO"}},{"cell_type":"markdown","source":["\\begin{equation}\n","\\nabla_\\mathbf{w} J(\\mathbf{w})= \\sum_{i=1}^N \\nabla_\\mathbf{w}S(x^i;\\mathbf{w}).\\left(S(x^i;\\mathbf{w}) - y^i \\right)\n","\\end{equation}"],"metadata":{"id":"PtCPhblKkjCN"}},{"cell_type":"markdown","source":["#### Example:"],"metadata":{"id":"6qGzNq08owkt"}},{"cell_type":"code","source":["n = 100\n","x = np.arange(n)\n","x = x.reshape(1,n)\n","y = np.random.randint(-50, 50, size=(1,n)) + 50. * np.log1p(x)\n","ytruth = 50. * np.log1p(x)"],"metadata":{"id":"_mmIhgufl8xu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure()\n","plt.plot(x.flatten(), y.flatten(), '--r.', markersize=9,label='data + linear interpolation')\n","plt.plot(x.flatten(), ytruth.flatten(),label='truth')\n","plt.legend()\n","plt.show()"],"metadata":{"id":"5SaP92QJoXSU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Regression objective:"],"metadata":{"id":"EGQvtwxmpVPb"}},{"cell_type":"code","source":["def objective(coeffs, monotoneMap, x, y_measured):\n","    monotoneMap.SetCoeffs(coeffs)\n","    map_of_x = monotoneMap.Evaluate(x)\n","    return 0.5*np.sum((map_of_x - y_measured)**2)/x.shape[1]\n","\n","def grad_objective(coeffs, monotoneMap, x, y_measured):\n","    monotoneMap.SetCoeffs(coeffs)\n","    map_of_x = monotoneMap.Evaluate(x)\n","    sensi = (map_of_x-y_measured)\n","    grad_map = tri_map.CoeffGrad(x, sensi) \n","    return np.sum(grad_map,1)/x.shape[1]"],"metadata":{"id":"4aMNOEv9pUdn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Setting map complexity and optimization of map coefficient:"],"metadata":{"id":"V5agFYRfo9yA"}},{"cell_type":"markdown","source":["Here the complexity (hence the approximation power) of the map can be set via setting the order of polynomials that we want to use."],"metadata":{"id":"RVCQHTlcpGBM"}},{"cell_type":"code","source":["x_norm = np.linspace(-4,4,n)\n","x_norm = x_norm.reshape(1,n)\n","\n","# Set-up map and initize map coefficients\n","opts = MapOptions()\n","map_order = 6\n","tri_map = CreateTriangular(1,1,map_order,opts)\n","coeffs = np.zeros(tri_map.numCoeffs)\n","tri_map.SetCoeffs(coeffs) #Initial value of coefficient is 0\n","\n","options={'gtol': 1e-4, 'disp': True}\n","\n","res = minimize(objective, tri_map.CoeffMap(), args=(tri_map,x_norm,y), jac=grad_objective, method='BFGS', options=options)"],"metadata":{"id":"3MM2tkfMuBVa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Plot map approximation"],"metadata":{"id":"dYW60eG_pXHA"}},{"cell_type":"code","source":["y_map = tri_map.Evaluate(x_norm)\n","\n","plt.figure()\n","plt.plot(x.flatten(), y.flatten(), '--r.', markersize=12,label='data + linear interpolation')\n","plt.plot(x.flatten(), ytruth.flatten(),label='truth')\n","plt.plot(x.flatten(), y_map.flatten(),'-g',label='map approximation')\n","\n","plt.legend()\n","plt.show()"],"metadata":{"id":"27JFWvpLybJ-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Compared to classical methods (see (https://scikit-learn.org/0.22/auto_examples/plot_isotonic_regression.html) transport maps regression allows to have continuous differentiable solution of the isotonic regression problem."],"metadata":{"id":"rsNUXcC6n615"}},{"cell_type":"markdown","source":["## Building map from samples drawn acording to unknown distribution "],"metadata":{"id":"pMInB3YoIAm1"}},{"cell_type":"markdown","source":["The objective is to build a transport map that characterize some density $\\pi(\\mathbf{x})$ given some samples $\\mathbf{x}^i, i\\in\\{1,...,N\\}$. "],"metadata":{"id":"PuZgQUbTNHFC"}},{"cell_type":"markdown","source":["### Generate training data"],"metadata":{"id":"5pfffZn4OjNP"}},{"cell_type":"markdown","source":["As an example we consider samples defined according some highly non-Gaussian 2D density:"],"metadata":{"id":"MLK2QYyFJbRS"}},{"cell_type":"code","source":["def sample_spiral_distribution(size):\n","    \n","    # First draw some rotation samples from a beta distribution, then scale \n","    # them to the range between -pi and +2pi\n","    seeds = scipy.stats.beta.rvs(\n","        a       = 7,\n","        b       = 3,\n","        size    = size)*2*np.pi-np.pi\n","    # Create a local copy of the rotations\n","    seeds_orig = copy.copy(seeds)\n","    # Re-normalize the rotations, then scale them to the range between [-3,+3]\n","    vals    = (seeds+np.pi)/(3*np.pi)*6-3\n","    # Plot the rotation samples on a straight spiral\n","    X       = np.column_stack((\n","        np.cos(seeds)[:,np.newaxis],\n","        np.sin(seeds)[:,np.newaxis]))*((1+seeds+np.pi)/(3*np.pi)*3)[:,np.newaxis]\n","    # Offset each sample along the spiral's normal vector by scaled Gaussian \n","    # noise\n","    X   += np.column_stack([\n","        np.cos(seeds_orig),\n","        np.sin(seeds_orig)])*(scipy.stats.norm.rvs(size=size)*scipy.stats.norm.pdf(vals))[:,np.newaxis]\n","    return X/2"],"metadata":{"id":"TA_mlaf8H9D1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Generate and plot training data $\\mathbf{x}^i, i\\in\\{1,...,N\\}$:"],"metadata":{"id":"ir7CVFVuN_v5"}},{"cell_type":"code","source":["# Training ensemble size\n","N = 10000\n","\n","# Draw that many samples\n","Xtrain = sample_spiral_distribution(N)\n","Xtrain = Xtrain.transpose()\n","\n","colors = np.arctan2(Xtrain[1,:],Xtrain[0,:])\n","\n","fig,ax = plt.subplots()\n","ax.scatter(Xtrain[0,:],Xtrain[1,:], c=colors, alpha=0.2, label='Target samples')\n","ax.set_aspect('equal', 'box')\n","plt.xlim([-1.75, 1.25])\n","plt.ylim([-1, 2])\n","\n","plt.show()"],"metadata":{"id":"OP805VIVKA24"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Defining objective function and its gradient:"],"metadata":{"id":"knIAemmsMJrw"}},{"cell_type":"markdown","source":["In order to use efficient gradient-based minimizer we define the objective and its gradient. The objective function for the map from samples is the log-likelihood that we want to maximize (or equivalently the negative log-likelihood that we want to minimize)."],"metadata":{"id":"HfYM8hjtOuLA"}},{"cell_type":"markdown","source":["The map from samples problem then writes:"],"metadata":{"id":"RENwUWmkPMP6"}},{"cell_type":"markdown","source":["\\begin{equation}\n","\\hat{S} = \\underset{S}{\\text{argmin }} -\\frac{1}{N} \\sum_{i=1}^N \\log S^\\sharp \\eta(\\mathbf{x}^i)\n","\\end{equation}\n","where $S$ is a transport map, $\\eta$ is a reference density (with known probability density function) and $\\mathbf{x}^i$ are training data points from density $\\pi$."],"metadata":{"id":"-tNZGdLisEb5"}},{"cell_type":"markdown","source":["This problem is reduced to finite dimension as we use a parametric formulation for the maps $S$. Hence we can transform the objective on the map $S$ to an objective on the parameters $\\mathbf{w}$ of a given class of map $S(.;\\mathbf{w})$."],"metadata":{"id":"_XyNJdAIPQnm"}},{"cell_type":"markdown","source":["If we use $\\eta$ as the standard normal distribution on $\\mathbb{R}^d$, and $S$ as a triangular map, we can decompose the objective function on the whole set of parameters $\\mathbf{w}$ in $d$ objective functions on set of parameters $\\mathbf{w}_k$ of each map component $S_k$, $k \\in \\{1,...,d\\}$."],"metadata":{"id":"h092SRGrQkoY"}},{"cell_type":"markdown","source":["The objective functions then read:"],"metadata":{"id":"bfdhP6h3QRc8"}},{"cell_type":"markdown","source":["\\begin{equation}\n","J_k(\\mathbf{w}_k) = - \\frac{1}{N}\\sum_{i=1}^N \\left( \\log\\eta\\left(S_k(\\mathbf{x}_{1:k}^i;\\mathbf{w}_k)\\right) + \\log \\frac{\\partial S_k(\\mathbf{x}_{1:k}^i;\\mathbf{w}_k)}{\\partial x_k}\\right), \\,\\,\\, \\mathbf{x}^i \\sim \\pi(\\mathbf{x}), \\,\\,\\, k=1,...,d\n","\\end{equation}"],"metadata":{"id":"uLvF3LbUtFXV"}},{"cell_type":"markdown","source":["and their gradients with respect to $\\mathbf{w}_k$, $k \\in \\{1,...,d\\}$:"],"metadata":{"id":"m9DSiH4qQVc7"}},{"cell_type":"markdown","source":["\\begin{equation}\n","\\nabla_{\\mathbf{w}_k}J_k(\\mathbf{w}_k) = - \\frac{1}{N}\\sum_{i=1}^N \\left(\\nabla_{\\mathbf{w}_k}S_k\n","(\\mathbf{x}_{1:k}^i;\\mathbf{w}_k).\\nabla_\\mathbf{z}\\log \\eta \\left(S_k\n","(\\mathbf{x}_{1:k}^i;\\mathbf{w}_k)\\right) - \\frac{\\partial \\nabla_{\\mathbf{w}_k}S_k(\\mathbf{x}_{1:k}^i;\\mathbf{w}_k)}{\\partial x_k} /\\frac{\\partial S_k(\\mathbf{x}_{1:k}^i;\\mathbf{w}_k)}{\\partial x_k}\\right), \\,\\,\\, \\mathbf{x}^i \\sim \\pi(\\mathbf{x})\n","\\end{equation}"],"metadata":{"id":"soDNl246vIAP"}},{"cell_type":"markdown","source":["### Minimizing objectives"],"metadata":{"id":"y_b1iMCgWGZF"}},{"cell_type":"code","source":["# Negative log likelihood objective\n","\n","rho1 = multivariate_normal(np.zeros(1),np.eye(1))\n","def obj(coeffs, tri_map,x):\n","    num_points = x.shape[1]\n","    tri_map.SetCoeffs(coeffs)\n","    map_of_x = tri_map.Evaluate(x)\n","    rho_of_map_of_x = rho1.logpdf(map_of_x.T)\n","    log_det = tri_map.LogDeterminant(x)\n","    return -np.sum(rho_of_map_of_x + log_det)/num_points\n","    \n","#Gradient of negative log likelihood objective\n","def grad_obj(coeffs, tri_map, x):\n","    num_points = x.shape[1]\n","    tri_map.SetCoeffs(coeffs)\n","    map_of_x = tri_map.Evaluate(x)\n","    grad_rho_of_map_of_x = -tri_map.CoeffGrad(x, map_of_x)\n","    grad_log_det = tri_map.LogDeterminantCoeffGrad(x)\n","    return -np.sum(grad_rho_of_map_of_x + grad_log_det, 1)/num_points"],"metadata":{"id":"dIpQCQcR2rMT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["For the considered problem, we can choose the paramatrization of each map components by setting the maximum order of polynomials used in the parametrization. We then use the [BFGS solver](https://docs.scipy.org/doc/scipy/reference/optimize.minimize-bfgs.html) to find polynomials/map coefficients."],"metadata":{"id":"OVwWn2pUSJgj"}},{"cell_type":"markdown","source":["For first component:"],"metadata":{"id":"I0G2AwKOSdMg"}},{"cell_type":"code","source":["total_order1 = 5\n","\n","# Create multi-index set:\n","fixed_mset1 = FixedMultiIndexSet(1,total_order1)\n","\n","# Set MapOptions and make map\n","opts = MapOptions()\n","\n","S1 = CreateComponent(fixed_mset1,opts)\n","\n","Xtrain1 = Xtrain[0,:].reshape(1,N) #use first coordinate samples only\n","\n","options={'gtol': 1e-4, 'disp': True}\n","res1 = minimize(obj, S1.CoeffMap(), args=(S1, Xtrain1), jac=grad_obj, method='BFGS', options=options)"],"metadata":{"id":"9X4lxodJ3HTf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["For second component:"],"metadata":{"id":"qyv5drfdS10k"}},{"cell_type":"code","source":["total_order2 = 5\n","\n","# Create multi-index set:\n","fixed_mset2 = FixedMultiIndexSet(2,total_order2)\n","\n","S2 = CreateComponent(fixed_mset2,opts)\n","\n","Xtrain2 = Xtrain #use both coordinates samples\n","\n","options={'gtol': 1e-3, 'disp': True}\n","res2 = minimize(obj, S2.CoeffMap(), args=(S2, Xtrain2), jac=grad_obj, method='BFGS', options=options)"],"metadata":{"id":"GW5wo-K95cjS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Once the map coefficients are found we can assess the quality of the map approximation by looking at the \"pushed\" samples $\\mathbf{z}^i=S(\\mathbf{x}^i)$. If the map approximation is good, the distribution of the $\\mathbf{z}^i$ should be close to a standard normal Gaussian.\n","\n"],"metadata":{"id":"XOtf3FYeM5x2"}},{"cell_type":"markdown","source":["### Accuracy check"],"metadata":{"id":"8r1Nme3pWN6t"}},{"cell_type":"code","source":["#Gather components in a Triangular Map object:\n","S = TriangularMap((S1,S2)) \n","S.SetCoeffs(np.concatenate((S1.CoeffMap(),S2.CoeffMap())))\n","\n","#Pushed samples should look like Gaussian samples:\n","Xpushed = S.Evaluate(Xtrain)\n","\n","#Plot\n","fig,ax = plt.subplots(1,2)\n","ax[0].scatter(Xtrain[0],Xtrain[1], c=colors, alpha=0.1, label='Training samples')\n","ax[0].set_title('Training samples')\n","ax[0].set_aspect('equal','box')\n","ax[0].set_xlim((-1.75, 1.25))\n","ax[0].set_ylim((-1, 2))\n","ax[1].scatter(Xpushed[0],Xpushed[1], c=colors, alpha=0.1, label='Pushed samples')\n","ax[1].set_title('Pushed samples')\n","ax[1].set_aspect('equal', 'box')\n","ax[1].set_xlim((-4.5, 4.5))\n","ax[1].set_ylim((-4.5, 4.5))\n","plt.show()"],"metadata":{"id":"A7HShTU5M5Bi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Applications"],"metadata":{"id":"bTBBikRCWZrk"}},{"cell_type":"markdown","source":["#### Generative modeling:"],"metadata":{"id":"V57q_g4KWf3h"}},{"cell_type":"markdown","source":["With an accurate transport map, we are now able to generate new samples that will be approximately be distributed in same way the training samples. \n","\n","Indeed, if we have $\\mathbf{z}^i \\sim \\mathcal{N}(0,1)$ then $S^{-1}(\\mathbf{z}^i)\\sim \\pi(\\mathbf{x})$:"],"metadata":{"id":"g3Bb6KaOT_II"}},{"cell_type":"code","source":["#Draw new samples from the learned distribution:\n","Znew = np.random.randn(2,5000)\n","\n","colors = np.arctan2(Znew[1,:],Znew[0,:])\n","Xnew = S.Inverse(Znew,Znew)"],"metadata":{"id":"TVUMpRr8T-Tg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Plot\n","fig,ax = plt.subplots(1,2)\n","ax[0].scatter(Znew[0,:],Znew[1,:], c=colors, alpha=0.2, label='Normal samples')\n","ax[0].set_title('Normal samples')\n","ax[0].set_aspect('equal', 'box')\n","ax[0].set_xlim((-4.5, 4.5))\n","ax[0].set_ylim((-4.5, 4.5))\n","ax[1].scatter(Xnew[0,:],Xnew[1,:], c=colors, alpha=0.2, label='New samples')\n","ax[1].set_title('New samples')\n","ax[1].set_aspect('equal', 'box')\n","ax[1].set_xlim((-1.75, 1.25))\n","ax[1].set_ylim((-1, 2))\n","plt.show()"],"metadata":{"id":"FozEaXNzTu4B"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Density estimation:"],"metadata":{"id":"291pJ_9YWllE"}},{"cell_type":"markdown","source":["Another use of the transport map $S$ is to estimate the probability function of $\\pi(\\mathbf{x})$. Indeed, in that case $\\pi(\\mathbf{x}) \\approx [S^\\sharp \\rho](\\mathbf{x})$."],"metadata":{"id":"XN9FpASduPvL"}},{"cell_type":"code","source":["def pullback_pdf(tri_map,rho,x):\n","  y = tri_map.Evaluate(x)\n","  log_pdf = rho.logpdf(y.T)+tri_map.LogDeterminant(x)\n","  return np.exp(log_pdf)\n","\n","Ngrid = 100\n","x = np.linspace(-1.5, 1., Ngrid)\n","y = np.linspace(-1, 1.5, Ngrid)\n","X, Y = np.meshgrid(x, y)\n","XY = np.vstack((X.flatten(),Y.flatten()))\n","\n","rho = multivariate_normal(np.zeros(2),np.eye(2))\n","\n","Z = pullback_pdf(S,rho,XY)\n","Z = Z.reshape(Ngrid,Ngrid)\n","\n","fig, ax = plt.subplots()\n","CS = ax.contour(X, Y, Z)\n","ax.set_aspect('equal', 'box')\n","plt.show()"],"metadata":{"id":"UUzBZR_AuPXy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Characterization of Bayesian inference posteriors via transport maps"],"metadata":{"id":"RDTPuoY2UA-j"}},{"cell_type":"markdown","source":["### Bayesian inference"],"metadata":{"id":"nM3TDiTyXiS9"}},{"cell_type":"markdown","source":["One other way to build transport maps is from unnormlized density. One situation where we know the probality density function up to a normalization constant is when modeling inverse problems with Bayesian inference."],"metadata":{"id":"61mDS9c_XESi"}},{"cell_type":"markdown","source":["For an inverse problem, the objective is to characterize the value of some parameters $\\boldsymbol{\\theta}$ of a given system, knowing some the value of some noisy observations $\\mathbf{y}$. "],"metadata":{"id":"05Xh-04NYSJp"}},{"cell_type":"markdown","source":["With Bayesian inference, the characterization of parameters $\\boldsymbol{\\theta}$ is done via a *posterior* density $\\pi(\\boldsymbol{\\theta}|\\mathbf{y})$. This density characterize the distribution of the parameters knowing the value of the observations."],"metadata":{"id":"hIA-PjEgY8bT"}},{"cell_type":"markdown","source":["The posterior can thus be decomposed by the product of two probabilistic quantities:\n","\n","\n","1.   The prior density $\\pi(\\boldsymbol{\\theta})$ which is use to enforce any *a priori* knowledge about the parameters.\n","2.   The likelihood function $\\pi(\\mathbf{y}|\\boldsymbol{\\theta})$. This quantity seen as a function of $\\boldsymbol{\\theta}$ gives the proability that the considered system produce the observation $\\mathbf{y}$ for a fixed value of $\\boldsymbol{\\theta}$. When the model that describes the system is known in closed form, the likelihood function is also knwon in closed form.\n","\n","Hence, the posterior density reads:\n","\\begin{equation}\n","\\pi(\\boldsymbol{\\theta}|\\mathbf{y}) = \\frac{1}{c} \\pi(\\mathbf{y}|\\boldsymbol{\\theta}).\\pi(\\boldsymbol{\\theta})\n","\\end{equation}\n","where $c$ is an unknown real constant that ensure that the product of the two quantities is a proper density.\n"],"metadata":{"id":"_yLFY5fmZi1t"}},{"cell_type":"markdown","source":["### Problem description"],"metadata":{"id":"MOmf-AOZcTIu"}},{"cell_type":"markdown","source":["Knowing the closed form of unnormalized posterior $\\bar{\\pi}(\\boldsymbol{\\theta} |\\mathbf{y})= \\pi(\\mathbf{y}|\\boldsymbol{\\theta}).\\pi(\\boldsymbol{\\theta})$, the objective is to draw samples from/approximate the posterior density.\n","\n"],"metadata":{"id":"uqDP_VK_chGe"}},{"cell_type":"markdown","source":["### Map from density"],"metadata":{"id":"INBIqSDpdf0F"}},{"cell_type":"markdown","source":["In order to characterize this posterior density, one method is to build a transport map."],"metadata":{"id":"B5WNmHEmddwz"}},{"cell_type":"markdown","source":["For the map from unnormalized density estimation, the objective function reads:"],"metadata":{"id":"8x05jV8yd6bk"}},{"cell_type":"markdown","source":["\\begin{equation}\n","J(\\mathbf{w}) = - \\frac{1}{N}\\sum_{i=1}^N \\left( \\log\\pi\\left(T(\\mathbf{z}^i;\\mathbf{w})\\right) + \\log  \\text{det }\\nabla_\\mathbf{z} T(\\mathbf{z}^i;\\mathbf{w})\\right), \\,\\,\\, \\mathbf{z}^i \\sim \\mathcal{N}(\\mathbf{0},\\mathbf{I}_d)\n","\\end{equation}\n","where $T$ is the transport map pushing forward the standard normal $mathcal{N}(\\mathbf{0},\\mathbf{I}_d)$ to the target density $\\pi(\\mathbf{x})$ (here the target density will be the posterior density)."],"metadata":{"id":"7xfNYRnneGet"}},{"cell_type":"markdown","source":["Gradient of the objective function in that case reads:"],"metadata":{"id":"kduvKQ6dePKz"}},{"cell_type":"markdown","source":["\\begin{equation}\n","\\nabla_\\mathbf{w} J(\\mathbf{w}) = - \\frac{1}{N}\\sum_{i=1}^N \\left( \\nabla_\\mathbf{w} T(\\mathbf{z}^i;\\mathbf{w}).\\nabla_\\mathbf{x}\\log\\pi\\left(T(\\mathbf{z}^i;\\mathbf{w})\\right) + \\nabla_{\\mathbf{w}}\\log  \\text{det }\\nabla_\\mathbf{z} T(\\mathbf{z}^i;\\mathbf{w})\\right), \\,\\,\\, \\mathbf{z}^i \\sim \\mathcal{N}(\\mathbf{0},\\mathbf{I}_d)\n","\\end{equation}"],"metadata":{"id":"lle8G9j7eN6n"}},{"cell_type":"markdown","source":["### Example: Biochemical Oxygen Demand (BOD) model\n","(example taken from https://arxiv.org/pdf/1602.05023.pdf)"],"metadata":{"id":"z4NQ5TfSfHbC"}},{"cell_type":"markdown","source":["#### Model:\n","The time dependent forward model is defined as:\n","\\begin{equation}\n","\\mathcal{B}(t) = A(1-\\exp(-Bt))+\\mathcal{E}\n","\\end{equation}\n","where:\n","\\begin{equation}\n","\\mathcal{E} \\sim \\mathcal{N}(0,1e-3)\n","\\end{equation}\n","\\begin{equation}\n","A = \\left[0.4 + 0.4\\left(1 + \\text{erf}\\left(\\frac{\\theta_1}{\\sqrt{2}} \\right)\\right) \\right]\n","\\end{equation}\n","\\begin{equation}\n","B = \\left[0.01 + 0.15\\left(1 + \\text{erf}\\left(\\frac{\\theta_2}{\\sqrt{2}} \\right)\\right) \\right]\n","\\end{equation}"],"metadata":{"id":"Z0bDu_mKfn0X"}},{"cell_type":"markdown","source":["The objective is to characterize the posterior density of parameters $\\boldsymbol{\\theta}=(\\theta_1,\\theta_2)$ knowing observation of the system at time $t=\\left\\{1,2,3,4,5 \\right\\}$ i.e. $\\mathbf{y} = (\\mathcal{B}(1),\\mathcal{B}(2),\\mathcal{B}(3),\\mathcal{B}(4),\\mathcal{B}(5))$."],"metadata":{"id":"gvUHx0CcheOn"}},{"cell_type":"markdown","source":["#### Deterministic model:"],"metadata":{"id":"vfE_x8ItUQpl"}},{"cell_type":"code","source":["def forward_model(p1,p2,t):\n","  A = 0.4+0.4*(1+scipy.special.erf(p1/np.sqrt(2)))\n","  B = 0.01+0.15*(1+scipy.special.erf(p2/np.sqrt(2)))\n","  out = A*(1-np.exp(-B*t))\n","  return out\n","\n","def grad_x_forward_model(p1,p2,t):\n","  A = 0.4+0.4*(1+scipy.special.erf(p1/np.sqrt(2)))\n","  B = 0.01+0.15*(1+scipy.special.erf(p2/np.sqrt(2)))\n","  dAdx1 = 0.31954*np.exp(-0.5*p1**2)\n","  dBdx2 = 0.119683*np.exp(-0.5*p2**2)\n","  dOutdx1 = dAdx1*(1-np.exp(-B*t))\n","  dOutdx2 = t*A*dBdx2*np.exp(-t*B)\n","  return np.vstack((dOutdx1,dOutdx2))"],"metadata":{"id":"FnTyhgc0T8lf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Likelihood functions:"],"metadata":{"id":"MXj4SeMvUfsj"}},{"cell_type":"code","source":["def log_likelihood(std_noise,t,yobs,p1,p2):\n","  y = forward_model(p1,p2,t)\n","  log_lkl = -np.log(np.sqrt(2*np.pi)*std_noise)-0.5*((y-yobs)/std_noise)**2\n","  return log_lkl\n","\n","def grad_x_log_likelihood(std_noise,t,yobs,p1,p2):\n","  y = forward_model(p1,p2,t)\n","  dydx = grad_x_forward_model(p1,p2,t)\n","  grad_x_lkl = (-1/std_noise**2)*(y - yobs)*dydx\n","  return grad_x_lkl"],"metadata":{"id":"B2PSr1BnUaun"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Unnormalized posterior density:"],"metadata":{"id":"vbqP5DiHUo2G"}},{"cell_type":"code","source":["def log_posterior(std_noise,std_prior1,std_prior2,list_t,list_yobs,p1,p2):\n","  log_prior1 = -np.log(np.sqrt(2*np.pi)*std_prior1)-0.5*(p1/std_prior1)**2\n","  log_prior2 = -np.log(np.sqrt(2*np.pi)*std_prior2)-0.5*(p2/std_prior2)**2\n","  log_posterior = log_prior1+log_prior2\n","  for k,t in enumerate(list_t):\n","    log_posterior += log_likelihood(std_noise,t,list_yobs[k],p1,p2)\n","  return log_posterior\n","\n","def grad_x_log_posterior(std_noise,std_prior1,std_prior2,list_t,list_yobs,p1,p2):\n","  grad_x1_prior = -(1/std_prior1**2)*(p1)\n","  grad_x2_prior = -(1/std_prior2**2)*(p2)\n","  grad_x_prior = np.vstack((grad_x1_prior,grad_x2_prior))\n","  grad_x_log_posterior = grad_x_prior \n","  for k,t in enumerate(list_t):\n","    grad_x_log_posterior += grad_x_log_likelihood(std_noise,t,list_yobs[k],p1,p2)\n","  return grad_x_log_posterior"],"metadata":{"id":"W5VhvAweUi6h"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Example with 5 observations"],"metadata":{"id":"M2c_CaNNU23t"}},{"cell_type":"code","source":["list_t = np.array([1,2,3,4,5])\n","list_yobs = np.array([0.18,0.32,0.42,0.49,0.54])\n","\n","std_noise = np.sqrt(1e-3);\n","std_prior1 = 1;\n","std_prior2 = 1;"],"metadata":{"id":"QB909TMQUryu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Visualization of the **unnormalized** posterior density:"],"metadata":{"id":"qCRL8_P1Wr8h"}},{"cell_type":"code","source":["Ngrid = 100\n","x = np.linspace(-0.5, 2.5, Ngrid)\n","y = np.linspace(-0.5, 2.5, Ngrid)\n","X, Y = np.meshgrid(x, y)\n","\n","Z = log_posterior(std_noise,std_prior1,std_prior2,list_t,list_yobs,X.flatten(),Y.flatten())\n","Z = np.exp(Z.reshape(Ngrid,Ngrid))\n","\n","\n","fig, ax = plt.subplots()\n","CS = ax.contour(X, Y, Z)\n","ax.set_xlabel(r'$\\theta_1$')\n","ax.set_ylabel(r'$\\theta_2$')\n","plt.show()"],"metadata":{"id":"aMRLess1bhK6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Definition of the objective function:"],"metadata":{"id":"guYpL1CNXTkm"}},{"cell_type":"code","source":["def grad_x_log_target(x):\n","  out = grad_x_log_posterior(std_noise,std_prior1,std_prior2,list_t,list_yobs,x[0,:],x[1,:])\n","  return out\n","\n","def log_target(x):\n","  out = log_posterior(std_noise,std_prior1,std_prior2,list_t,list_yobs,x[0,:],x[1,:])\n","  return out\n","\n","def obj(coeffs, tri_map,x):\n","    num_points = x.shape[1]\n","    tri_map.SetCoeffs(coeffs)\n","    map_of_x = tri_map.Evaluate(x)\n","    rho_of_map_of_x = log_target(map_of_x)\n","    log_det = tri_map.LogDeterminant(x)\n","    return -np.sum(rho_of_map_of_x + log_det)/num_points\n","\n","def grad_obj(coeffs, tri_map, x):\n","    num_points = x.shape[1]\n","    tri_map.SetCoeffs(coeffs)\n","    map_of_x = tri_map.Evaluate(x)\n","    sensi = grad_x_log_target(map_of_x)\n","    grad_rho_of_map_of_x = tri_map.CoeffGrad(x, sensi) \n","    grad_log_det = tri_map.LogDeterminantCoeffGrad(x)\n","    return -np.sum(grad_rho_of_map_of_x + grad_log_det, 1)/num_points"],"metadata":{"id":"mVDIKKPHXSu0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Draw reference samples\n","N=10000\n","Xtrain = np.random.randn(2,N)"],"metadata":{"id":"JKOzmtbEXfqB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Map parametrization and optimization:"],"metadata":{"id":"DHkXWX0NXm1_"}},{"cell_type":"code","source":["# Set-up map and initize map coefficients\n","opts = MapOptions()\n","\n","map_order = 3\n","tri_map = CreateTriangular(2,2,map_order,opts)\n","coeffs = np.zeros(tri_map.numCoeffs)\n","tri_map.SetCoeffs(coeffs)\n","\n","options={'gtol': 1e-2, 'disp': True}\n","res = minimize(obj, tri_map.CoeffMap(), args=(tri_map, Xtrain), jac=grad_obj, method='BFGS', options=options)\n"],"metadata":{"id":"R-ER6wtwXhUc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Comparing contours of approximate posterior and true unnormalized posterior"],"metadata":{"id":"Ju10kojxjc9w"}},{"cell_type":"code","source":["def push_forward_pdf(tri_map,rho,x):\n","  xinv = tri_map.Inverse(x,x)\n","  log_det_grad_x_inverse = - tri_map.LogDeterminant(xinv)\n","  log_pdf = rho.logpdf(xinv.T)+log_det_grad_x_inverse\n","  return np.exp(log_pdf)"],"metadata":{"id":"Eu0zwM89Z8fG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Ngrid = 100\n","x = np.linspace(-0.5, 1.3, Ngrid)\n","y = np.linspace(-0.2, 2.3, Ngrid)\n","xx, yy = np.meshgrid(x, y)\n","\n","xx_eval = np.vstack((xx.flatten(),yy.flatten()))\n","Z2 = push_forward_pdf(tri_map,rho,xx_eval)\n","Z2 = Z2.reshape(Ngrid,Ngrid)\n","\n","Z = log_posterior(std_noise,std_prior1,std_prior2,list_t,list_yobs,xx.flatten(),yy.flatten())\n","Z = np.exp(Z.reshape(Ngrid,Ngrid))\n","\n","fig, ax = plt.subplots()\n","CS1 = ax.contour(xx, yy, Z)\n","CS2 = ax.contour(xx, yy, Z2,linestyles='dashed')\n","ax.set_xlabel(r'$\\theta_1$')\n","ax.set_ylabel(r'$\\theta_2$')\n","h1,_ = CS1.legend_elements()\n","h2,_ = CS2.legend_elements()\n","ax.legend([h1[0], h2[0]], ['Unnormilzed posterior', 'TM approximation'])\n","plt.show()\n","np.savetxt('Zpost.txt',Z)"],"metadata":{"id":"F-5wPAGDYPv7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Drawing samples from approximate posterior\n","\n","Once the transport map from reference to unnormalized posterior is estimated it can be used to sample from the posterior to characterize the Bayesian inference solution. "],"metadata":{"id":"tO12yfcI-8SQ"}},{"cell_type":"code","source":["Znew = np.random.randn(2,15000)\n","colors = np.arctan2(Znew[1,:],Znew[0,:])\n","\n","Xpost = tri_map.Evaluate(Znew)\n","fig,ax = plt.subplots()\n","ax.scatter(Xpost[0,:],Xpost[1,:], c=colors, alpha=0.2, label='Posterior samples')\n","ax.set_aspect('equal', 'box')\n","ax.set_xlabel(r'$\\theta_1$')\n","ax.set_xlabel(r'$\\theta_2$')\n","ax.legend()\n","plt.show()\n","\n","np.savetxt('Xpost.txt',Xpost)"],"metadata":{"id":"ZqqSOnUN-_MG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Mean a posteriori:"],"metadata":{"id":"cpeZF5TjCYOX"}},{"cell_type":"code","source":["X_mean = np.mean(Xpost,1)\n","print('Mean a posteriori: '+str(X_mean))"],"metadata":{"id":"VZDvAgtUCcPU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["One-dimensional marginals histograms:"],"metadata":{"id":"kvedTHn0j2SX"}},{"cell_type":"code","source":["fig, ax = plt.subplots(1,2)\n","ax[0].hist(Xpost[0,:], 50, alpha=0.5, density=True)\n","ax[0].set_xlabel(r'$\\theta_1$')\n","ax[1].hist(Xpost[1,:], 50, alpha=0.5, density=True)\n","ax[1].set_xlabel(r'$\\theta_2$')\n","plt.show()"],"metadata":{"id":"n-kH0LBqB8d4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Posterior density from samples (Likelihood Free Inference)"],"metadata":{"id":"TBiFp-wRKBPu"}},{"cell_type":"markdown","source":["One other way to characterize the Bayesian inference solution is to use the property of triangular maps that allows characterization of conditional densities."],"metadata":{"id":"8nPVPr-TfT3B"}},{"cell_type":"markdown","source":["Indeed, let's consider two random vectors $\\mathbf{x}$ and $\\mathbf{y}$. If the triangular map $S$ with structure:\n","\\begin{equation}\n","S=\n","\\begin{bmatrix}\n","S_y(\\mathbf{y})\\\\\n","S_x(\\mathbf{y},\\mathbf{x})\n","\\end{bmatrix}\n","\\end{equation}\n","is defined as:\n","\\begin{equation}\n","\\left[S^\\sharp \\eta\\right](\\mathbf{y},\\mathbf{x}) = \\pi(\\mathbf{y},\\mathbf{x}),\n","\\end{equation}\n","then the conditional density $\\pi(\\mathbf{x}|\\mathbf{y})$ can be defined with the lower block of $S$ as:\n","\\begin{equation}\n","S_x(\\mathbf{y},.)^\\sharp \\eta(\\mathbf{x}) = \\pi(\\mathbf{x}|\\mathbf{y})\n","\\end{equation}"],"metadata":{"id":"o77hvdmCfyCy"}},{"cell_type":"markdown","source":["Applied to posterior characterization, this leads to computing the transport map between the joint density of observation and parameters $\\pi(\\mathbf{y},\\boldsymbol{\\theta})$ and reference density $\\eta$."],"metadata":{"id":"nI9TQDhkiaeJ"}},{"cell_type":"markdown","source":["### Generation of joint samples $\\{\\mathbf{y}^i,\\boldsymbol{\\theta}^i \\}$"],"metadata":{"id":"HW2pUi-0lhzp"}},{"cell_type":"code","source":["N = 10000\n","X = np.random.randn(2,N)\n","\n","Y = np.zeros((5,N))\n","\n","for t in range(5):\n","  Y[t,:]=forward_model(X[0,:],X[1,:],t+1)+std_noise*np.random.randn(1,N)\n","\n","YX = np.vstack((Y,X))"],"metadata":{"id":"9Pwpr9YcU2KN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Normalization of samples\n","\n","It is often beneficial to normalized training samples before computing a transport map. In order to that we simply define a linear map defined by mean and coviarance of training samples. "],"metadata":{"id":"hIM38yyMl1E0"}},{"cell_type":"code","source":["mu = np.mean(YX,1)\n","std_data = np.std(YX,1)\n","\n","Linv = np.diag(1/std_data)\n","mu_inv = - np.dot(Linv,mu)\n","\n","YXnorm = mu_inv.reshape(-1,1)+np.dot(Linv,YX)"],"metadata":{"id":"IrG6u8nilzSI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Finding lower block map components"],"metadata":{"id":"fclpzh3MmO5E"}},{"cell_type":"markdown","source":["To characterize the posterior density from the map that characterizes the joint density, we actually only need to compute the map components corresponding to parameters. Hence in our Bayesian example, from the joint map of dimension 7 (5 observations + 2 parameters), we only need to compute the last two compoenents. Then, we can use the property of decomposability of the minimization problems to separately find map components 6 and 7."],"metadata":{"id":"dA3TtR8Gmfaz"}},{"cell_type":"code","source":["# Negative log likelihood objective\n","\n","rho1 = multivariate_normal(np.zeros(1),np.eye(1))\n","def obj(coeffs, tri_map,x):\n","    num_points = x.shape[1]\n","    tri_map.SetCoeffs(coeffs)\n","    map_of_x = tri_map.Evaluate(x)\n","    rho_of_map_of_x = rho1.logpdf(map_of_x.T)\n","    log_det = tri_map.LogDeterminant(x)\n","    return -np.sum(rho_of_map_of_x + log_det)/num_points\n","\n","def grad_obj(coeffs, tri_map, x):\n","    num_points = x.shape[1]\n","    tri_map.SetCoeffs(coeffs)\n","    map_of_x = tri_map.Evaluate(x)\n","    grad_rho_of_map_of_x = -tri_map.CoeffGrad(x, map_of_x)\n","    grad_log_det = tri_map.LogDeterminantCoeffGrad(x)\n","    return -np.sum(grad_rho_of_map_of_x + grad_log_det, 1)/num_points"],"metadata":{"id":"C61zK_i8vWGI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Component 6:"],"metadata":{"id":"mPTsbfNGn611"}},{"cell_type":"code","source":["#multis6 = np.loadtxt('/content/multis6.txt',delimiter=',')\n","#mset6 = MultiIndexSet(multis6)\n","#fixed_mest6 = mset6.fix(True)\n","\n","fixed_mset6 = FixedMultiIndexSet(6,4)\n","\n","S6 = CreateComponent(fixed_mset6,opts)\n","Xtrain = YXnorm[:6,:]\n","\n","options={'gtol': 1e-2, 'disp': True}\n","res2 = minimize(obj, S6.CoeffMap(), args=(S6, Xtrain), jac=grad_obj, method='BFGS', options=options)"],"metadata":{"id":"hwJEMipk6Wku"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Component 7:"],"metadata":{"id":"4bnE9gZooB2y"}},{"cell_type":"code","source":["#multis7 = np.loadtxt('/content/multis7.txt',delimiter=',')\n","#mset7 = MultiIndexSet(multis7)\n","#fixed_mest7 = mset7.fix(True)\n","\n","fixed_mset7 = FixedMultiIndexSet(7,3)\n","\n","S7 = CreateComponent(fixed_mset7,opts)\n","\n","Xtrain = YXnorm\n","\n","options={'gtol': 1e-2, 'disp': True}\n","res2 = minimize(obj, S7.CoeffMap(), args=(S7, Xtrain), jac=grad_obj, method='BFGS', options=options)"],"metadata":{"id":"S4_Pxj5YKeaa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cond_map = TriangularMap((S6,S7))\n","cond_map.SetCoeffs(np.concatenate((S6.CoeffMap(),S7.CoeffMap())))"],"metadata":{"id":"jHO5rmLC1tDk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def cond_composed_pullback_pdf(tri_map,mu,L,rho,y,x):\n","  y = y.reshape(-1,1)*np.ones((len(y),x.shape[1]))\n","  yx = np.vstack((y,x))\n","  Lyx = mu.reshape(-1,1)+np.dot(L,yx)\n","  eval = tri_map.Evaluate(Lyx)\n","  log_pdf = rho.logpdf(eval.T)+tri_map.LogDeterminant(yx)+np.log(np.linalg.det(Linv))\n","  return np.exp(log_pdf)"],"metadata":{"id":"hkMDUgrxzQwa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Ngrid = 100\n","x = np.linspace(-0.7, 2.5, Ngrid)\n","y = np.linspace(-0.7, 2.5, Ngrid)\n","xx, yy = np.meshgrid(x, y)\n","\n","xx_eval = np.vstack((xx.flatten(),yy.flatten()))\n","Z2 = cond_composed_pullback_pdf(cond_map,mu_inv,Linv,rho,list_yobs,xx_eval)\n","Z2 = Z2.reshape(Ngrid,Ngrid)\n","\n","Z = log_posterior(std_noise,std_prior1,std_prior2,list_t,list_yobs,xx.flatten(),yy.flatten())\n","Z = np.exp(Z.reshape(Ngrid,Ngrid))\n","\n","fig, ax = plt.subplots()\n","CS1 = ax.contour(xx, yy, Z)\n","CS2 = ax.contour(xx, yy, Z2,linestyles='dashed')\n","h1,_ = CS1.legend_elements()\n","h2,_ = CS2.legend_elements()\n","ax.legend([h1[0], h2[0]], ['Unnormilzed posterior', 'TM approximation'])\n","ax.set_xlabel(r'$\\theta_1$')\n","ax.set_ylabel(r'$\\theta_2$')\n","plt.show()"],"metadata":{"id":"yl5WXoa5DGQE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Advantages of this method is that we don't need to explicity know the model, the likelihood function its derivates etc. The likelihood function is implicitly learned directly from samples. One drawback though is that map components are higher dimensional than in the map from density case."],"metadata":{"id":"I6bniYDvoJgl"}}]}