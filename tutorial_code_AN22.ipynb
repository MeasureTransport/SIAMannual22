{"cells":[{"cell_type":"markdown","metadata":{"id":"4XFhZMWrD3kC"},"source":["# [MS65] Tutorials for Students: An Introduction to Sampling with Measure Transport"]},{"cell_type":"markdown","metadata":{"id":"b64U1f9cETls"},"source":["## Software installation (~5min)"]},{"cell_type":"markdown","metadata":{"id":"Ff0WEclMGKPv"},"source":["Import useful python packages:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JF00-Y5rFbpV"},"outputs":[],"source":["from mpart import *\n","import numpy as np\n","from scipy.optimize import minimize\n","from scipy.stats import norm\n","import matplotlib.pyplot as plt\n","import scipy.stats\n","import copy\n","from scipy.stats import multivariate_normal\n","import time\n","\n","plt.rcParams['figure.dpi'] = 120\n","plt.rcParams['savefig.dpi'] = 300"]},{"cell_type":"markdown","metadata":{"id":"NNQGXtzSfzV4"},"source":["## The transport map object: a parametrized class of monotone functions"]},{"cell_type":"markdown","metadata":{"id":"9uEWGMfc2IpL"},"source":["The first property of triangular transport maps that we can highlight is that each component is monotone with respect to the last variable. In order to parametrized the class of monotone functions we use the concept of rectifier (see https://arxiv.org/pdf/2009.10303.pdf). The objective of the rectifier is to transform any multivariate functions to a monotone function with respect to the last variable. The rectifier $R(f)$ of a function $f:\\mathbb{R}^d â†’ \\mathbb{R}^d$ is defined as follow:\n","\\begin{equation}\n","R(f)(x_{1:d}) = f(x_1,...,x_{d-1},0) + \\int_0^{x_d} g\\left(\\partial_{x_d} f(x_1,...,x_{d-1},t\\right) \\text{d}t\n","\\end{equation}\n","where $g$ is a real positive function e.g.:\n","\\begin{equation}\n","g(\\xi) = \\frac{\\log(1+2^\\xi)}{\\log(2)}\n","\\end{equation}"]},{"cell_type":"markdown","metadata":{"id":"fDfJf5Bo4v8M"},"source":["### A collection of polynomials and their corresponding monotone functions in 1D:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yDdbu2l2lLmL"},"outputs":[],"source":["n = 100\n","x = np.linspace(-2,2,n)\n","x = x.reshape(1,n)\n","\n","opts = MapOptions()\n","\n","\n","fig,ax = plt.subplots(1,2)\n","for k in range(8):\n","  map_order = np.random.randint(1,6)\n","  map_1d = CreateTriangular(1,1,map_order,opts)\n","  coeffs = 0.5*np.random.randn(map_1d.numCoeffs)\n","  map_1d.SetCoeffs(coeffs) \n","  f1 = map_1d.GetComponent(0).GetBaseFunction()\n","  f1.SetCoeffs(coeffs)\n","  Sy = map_1d.Evaluate(x)\n","  y = f1.Evaluate(x)\n","  ax[0].plot(x.flatten(),y.flatten())\n","  ax[0].set_title('Polynomials')\n","  ax[1].plot(x.flatten(),Sy.flatten())\n","  ax[1].set_title('Monotone functions')\n","  del(f1)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"HlAy_J9Of7bM"},"source":["### Use monotone parametrization for regression \n","\n","One direct use of this property given by map parametrization is to model monotone function from noisy data. This field also called isotonic regression can be solved via minimization of the following objective function:"]},{"cell_type":"markdown","metadata":{"id":"oKTfJ7iJjV1f"},"source":["\\begin{equation}\n","J(\\mathbf{w})= \\frac{1}{2} \\sum_{i=1}^N \\left(S(x^i;\\mathbf{w}) - y^i \\right)^2\n","\\end{equation}"]},{"cell_type":"markdown","metadata":{"id":"paZJUHgHoQp0"},"source":["where is a monotone 1D map with parameters (polynomial coefficients) $\\mathbf{w}$ and $y^i$ are noisy observations."]},{"cell_type":"markdown","metadata":{"id":"Udc1yT_noqKO"},"source":["The corresponding gradient objective reads:"]},{"cell_type":"markdown","metadata":{"id":"PtCPhblKkjCN"},"source":["\\begin{equation}\n","\\nabla_\\mathbf{w} J(\\mathbf{w})= \\sum_{i=1}^N \\nabla_\\mathbf{w}S(x^i;\\mathbf{w}).\\left(S(x^i;\\mathbf{w}) - y^i \\right)\n","\\end{equation}"]},{"cell_type":"markdown","metadata":{"id":"6qGzNq08owkt"},"source":["#### Example:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_mmIhgufl8xu"},"outputs":[],"source":["n = 100\n","x = np.arange(n)\n","x = x.reshape(1,n)\n","y = np.random.randint(-50, 50, size=(1,n)) + 50. * np.log1p(x)\n","ytruth = 50. * np.log1p(x)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5SaP92QJoXSU"},"outputs":[],"source":["plt.figure()\n","plt.plot(x.flatten(), y.flatten(), '--r.', markersize=9,label='data + linear interpolation')\n","plt.plot(x.flatten(), ytruth.flatten(),label='truth')\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"EGQvtwxmpVPb"},"source":["#### Regression objective:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4aMNOEv9pUdn"},"outputs":[],"source":["def objective(coeffs, monotoneMap, x, y_measured):\n","    monotoneMap.SetCoeffs(coeffs)\n","    map_of_x = monotoneMap.Evaluate(x)\n","    return 0.5*np.sum((map_of_x - y_measured)**2)/x.shape[1]\n","\n","def grad_objective(coeffs, monotoneMap, x, y_measured):\n","    monotoneMap.SetCoeffs(coeffs)\n","    map_of_x = monotoneMap.Evaluate(x)\n","    sensi = (map_of_x-y_measured)\n","    grad_map = tri_map.CoeffGrad(x, sensi) \n","    return np.sum(grad_map,1)/x.shape[1]"]},{"cell_type":"markdown","metadata":{"id":"V5agFYRfo9yA"},"source":["#### Setting map complexity and optimization of map coefficient:"]},{"cell_type":"markdown","metadata":{"id":"RVCQHTlcpGBM"},"source":["Here the complexity (hence the approximation power) of the map can be set via setting the order of polynomials that we want to use."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3MM2tkfMuBVa"},"outputs":[],"source":["x_norm = np.linspace(-4,4,n)\n","x_norm = x_norm.reshape(1,n)\n","\n","# Set-up map and initize map coefficients\n","opts = MapOptions()\n","map_order = 6\n","tri_map = CreateTriangular(1,1,map_order,opts)\n","coeffs = np.zeros(tri_map.numCoeffs)\n","tri_map.SetCoeffs(coeffs) #Initial value of coefficient is 0\n","\n","options={'gtol': 1e-4, 'disp': True}\n","\n","res = minimize(objective, tri_map.CoeffMap(), args=(tri_map,x_norm,y), jac=grad_objective, method='BFGS', options=options)"]},{"cell_type":"markdown","metadata":{"id":"dYW60eG_pXHA"},"source":["### Plot map approximation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"27JFWvpLybJ-"},"outputs":[],"source":["y_map = tri_map.Evaluate(x_norm)\n","\n","plt.figure()\n","plt.plot(x.flatten(), y.flatten(), '--r.', markersize=12,label='data + linear interpolation')\n","plt.plot(x.flatten(), ytruth.flatten(),label='truth')\n","plt.plot(x.flatten(), y_map.flatten(),'-g',label='map approximation')\n","\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"rsNUXcC6n615"},"source":["Compared to classical methods (see (https://scikit-learn.org/0.22/auto_examples/plot_isotonic_regression.html) transport maps regression allows to have continuous differentiable solution of the isotonic regression problem."]},{"cell_type":"markdown","metadata":{"id":"pMInB3YoIAm1"},"source":["## Building map from samples drawn acording to unknown distribution "]},{"cell_type":"markdown","metadata":{"id":"PuZgQUbTNHFC"},"source":["The objective is to build a transport map that characterize some density $\\pi(\\mathbf{x})$ given some samples $\\mathbf{x}^i, i\\in\\{1,...,N\\}$. "]},{"cell_type":"markdown","metadata":{"id":"5pfffZn4OjNP"},"source":["### Generate training data"]},{"cell_type":"markdown","metadata":{"id":"MLK2QYyFJbRS"},"source":["As an example we consider samples defined according some highly non-Gaussian 2D density:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TA_mlaf8H9D1"},"outputs":[],"source":["def sample_spiral_distribution(size):\n","    \n","    # First draw some rotation samples from a beta distribution, then scale \n","    # them to the range between -pi and +2pi\n","    seeds = scipy.stats.beta.rvs(\n","        a       = 7,\n","        b       = 3,\n","        size    = size)*2*np.pi-np.pi\n","    # Create a local copy of the rotations\n","    seeds_orig = copy.copy(seeds)\n","    # Re-normalize the rotations, then scale them to the range between [-3,+3]\n","    vals    = (seeds+np.pi)/(3*np.pi)*6-3\n","    # Plot the rotation samples on a straight spiral\n","    X       = np.column_stack((\n","        np.cos(seeds)[:,np.newaxis],\n","        np.sin(seeds)[:,np.newaxis]))*((1+seeds+np.pi)/(3*np.pi)*3)[:,np.newaxis]\n","    # Offset each sample along the spiral's normal vector by scaled Gaussian \n","    # noise\n","    X   += np.column_stack([\n","        np.cos(seeds_orig),\n","        np.sin(seeds_orig)])*(scipy.stats.norm.rvs(size=size)*scipy.stats.norm.pdf(vals))[:,np.newaxis]\n","    return X/2"]},{"cell_type":"markdown","metadata":{"id":"ir7CVFVuN_v5"},"source":["Generate and plot training data $\\mathbf{x}^i, i\\in\\{1,...,N\\}$:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OP805VIVKA24"},"outputs":[],"source":["# Training ensemble size\n","N = 10000\n","\n","# Draw that many samples\n","Xtrain = sample_spiral_distribution(N)\n","Xtrain = Xtrain.transpose()\n","\n","colors = np.arctan2(Xtrain[1,:],Xtrain[0,:])\n","\n","fig,ax = plt.subplots()\n","ax.scatter(Xtrain[0,:],Xtrain[1,:], c=colors, alpha=0.2, label='Target samples')\n","ax.set_aspect('equal', 'box')\n","plt.xlim([-1.75, 1.25])\n","plt.ylim([-1, 2])\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"knIAemmsMJrw"},"source":["### Defining objective function and its gradient:"]},{"cell_type":"markdown","metadata":{"id":"HfYM8hjtOuLA"},"source":["In order to use efficient gradient-based minimizer we define the objective and its gradient. The objective function for the map from samples is the log-likelihood that we want to maximize (or equivalently the negative log-likelihood that we want to minimize)."]},{"cell_type":"markdown","metadata":{"id":"RENwUWmkPMP6"},"source":["The map from samples problem then writes:"]},{"cell_type":"markdown","metadata":{"id":"-tNZGdLisEb5"},"source":["\\begin{equation}\n","\\hat{S} = \\underset{S}{\\text{argmin }} -\\frac{1}{N} \\sum_{i=1}^N \\log S^\\sharp \\eta(\\mathbf{x}^i)\n","\\end{equation}\n","where $S$ is a transport map, $\\eta$ is a reference density (with known probability density function) and $\\mathbf{x}^i$ are training data points from density $\\pi$."]},{"cell_type":"markdown","metadata":{"id":"_XyNJdAIPQnm"},"source":["This problem is reduced to finite dimension as we use a parametric formulation for the maps $S$. Hence we can transform the objective on the map $S$ to an objective on the parameters $\\mathbf{w}$ of a given class of map $S(.;\\mathbf{w})$."]},{"cell_type":"markdown","metadata":{"id":"h092SRGrQkoY"},"source":["If we use $\\eta$ as the standard normal distribution on $\\mathbb{R}^d$, and $S$ as a triangular map, we can decompose the objective function on the whole set of parameters $\\mathbf{w}$ in $d$ objective functions on set of parameters $\\mathbf{w}_k$ of each map component $S_k$, $k \\in \\{1,...,d\\}$."]},{"cell_type":"markdown","metadata":{"id":"bfdhP6h3QRc8"},"source":["The objective functions then read:"]},{"cell_type":"markdown","metadata":{"id":"uLvF3LbUtFXV"},"source":["\\begin{equation}\n","J_k(\\mathbf{w}_k) = - \\frac{1}{N}\\sum_{i=1}^N \\left( \\log\\eta\\left(S_k(\\mathbf{x}_{1:k}^i;\\mathbf{w}_k)\\right) + \\log \\frac{\\partial S_k(\\mathbf{x}_{1:k}^i;\\mathbf{w}_k)}{\\partial x_k}\\right), \\,\\,\\, \\mathbf{x}^i \\sim \\pi(\\mathbf{x}), \\,\\,\\, k=1,...,d\n","\\end{equation}"]},{"cell_type":"markdown","metadata":{"id":"m9DSiH4qQVc7"},"source":["and their gradients with respect to $\\mathbf{w}_k$, $k \\in \\{1,...,d\\}$:"]},{"cell_type":"markdown","metadata":{"id":"soDNl246vIAP"},"source":["\\begin{equation}\n","\\nabla_{\\mathbf{w}_k}J_k(\\mathbf{w}_k) = - \\frac{1}{N}\\sum_{i=1}^N \\left(\\nabla_{\\mathbf{w}_k}S_k\n","(\\mathbf{x}_{1:k}^i;\\mathbf{w}_k).\\nabla_\\mathbf{z}\\log \\eta \\left(S_k\n","(\\mathbf{x}_{1:k}^i;\\mathbf{w}_k)\\right) - \\frac{\\partial \\nabla_{\\mathbf{w}_k}S_k(\\mathbf{x}_{1:k}^i;\\mathbf{w}_k)}{\\partial x_k} /\\frac{\\partial S_k(\\mathbf{x}_{1:k}^i;\\mathbf{w}_k)}{\\partial x_k}\\right), \\,\\,\\, \\mathbf{x}^i \\sim \\pi(\\mathbf{x})\n","\\end{equation}"]},{"cell_type":"markdown","metadata":{"id":"y_b1iMCgWGZF"},"source":["### Minimizing objectives"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dIpQCQcR2rMT"},"outputs":[],"source":["# Negative log likelihood objective\n","\n","rho1 = multivariate_normal(np.zeros(1),np.eye(1))\n","def obj(coeffs, tri_map,x):\n","    num_points = x.shape[1]\n","    tri_map.SetCoeffs(coeffs)\n","    map_of_x = tri_map.Evaluate(x)\n","    rho_of_map_of_x = rho1.logpdf(map_of_x.T)\n","    log_det = tri_map.LogDeterminant(x)\n","    return -np.sum(rho_of_map_of_x + log_det)/num_points\n","    \n","#Gradient of negative log likelihood objective\n","def grad_obj(coeffs, tri_map, x):\n","    num_points = x.shape[1]\n","    tri_map.SetCoeffs(coeffs)\n","    map_of_x = tri_map.Evaluate(x)\n","    grad_rho_of_map_of_x = -tri_map.CoeffGrad(x, map_of_x)\n","    grad_log_det = tri_map.LogDeterminantCoeffGrad(x)\n","    return -np.sum(grad_rho_of_map_of_x + grad_log_det, 1)/num_points"]},{"cell_type":"markdown","metadata":{"id":"OVwWn2pUSJgj"},"source":["For the considered problem, we can choose the paramatrization of each map components by setting the maximum order of polynomials used in the parametrization. We then use the [BFGS solver](https://docs.scipy.org/doc/scipy/reference/optimize.minimize-bfgs.html) to find polynomials/map coefficients."]},{"cell_type":"markdown","metadata":{"id":"I0G2AwKOSdMg"},"source":["For first component:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9X4lxodJ3HTf"},"outputs":[],"source":["total_order1 = 5\n","\n","# Create multi-index set:\n","fixed_mset1 = FixedMultiIndexSet(1,total_order1)\n","\n","# Set MapOptions and make map\n","opts = MapOptions()\n","\n","S1 = CreateComponent(fixed_mset1,opts)\n","\n","Xtrain1 = Xtrain[0,:].reshape(1,N) #use first coordinate samples only\n","\n","options={'gtol': 1e-4, 'disp': True}\n","res1 = minimize(obj, S1.CoeffMap(), args=(S1, Xtrain1), jac=grad_obj, method='BFGS', options=options)"]},{"cell_type":"markdown","metadata":{"id":"qyv5drfdS10k"},"source":["For second component:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GW5wo-K95cjS"},"outputs":[],"source":["total_order2 = 5\n","\n","# Create multi-index set:\n","fixed_mset2 = FixedMultiIndexSet(2,total_order2)\n","\n","S2 = CreateComponent(fixed_mset2,opts)\n","\n","Xtrain2 = Xtrain #use both coordinates samples\n","\n","options={'gtol': 1e-3, 'disp': True}\n","res2 = minimize(obj, S2.CoeffMap(), args=(S2, Xtrain2), jac=grad_obj, method='BFGS', options=options)"]},{"cell_type":"markdown","metadata":{"id":"XOtf3FYeM5x2"},"source":["Once the map coefficients are found we can assess the quality of the map approximation by looking at the \"pushed\" samples $\\mathbf{z}^i=S(\\mathbf{x}^i)$. If the map approximation is good, the distribution of the $\\mathbf{z}^i$ should be close to a standard normal Gaussian.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"8r1Nme3pWN6t"},"source":["### Accuracy check"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A7HShTU5M5Bi"},"outputs":[],"source":["#Gather components in a Triangular Map object:\n","S = TriangularMap((S1,S2)) \n","S.SetCoeffs(np.concatenate((S1.CoeffMap(),S2.CoeffMap())))\n","\n","#Pushed samples should look like Gaussian samples:\n","Xpushed = S.Evaluate(Xtrain)\n","\n","#Plot\n","fig,ax = plt.subplots(1,2)\n","ax[0].scatter(Xtrain[0],Xtrain[1], c=colors, alpha=0.1, label='Training samples')\n","ax[0].set_title('Training samples')\n","ax[0].set_aspect('equal','box')\n","ax[0].set_xlim((-1.75, 1.25))\n","ax[0].set_ylim((-1, 2))\n","ax[1].scatter(Xpushed[0],Xpushed[1], c=colors, alpha=0.1, label='Pushed samples')\n","ax[1].set_title('Pushed samples')\n","ax[1].set_aspect('equal', 'box')\n","ax[1].set_xlim((-4.5, 4.5))\n","ax[1].set_ylim((-4.5, 4.5))\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"bTBBikRCWZrk"},"source":["### Applications"]},{"cell_type":"markdown","metadata":{"id":"V57q_g4KWf3h"},"source":["#### Generative modeling:"]},{"cell_type":"markdown","metadata":{"id":"g3Bb6KaOT_II"},"source":["With an accurate transport map, we are now able to generate new samples that will be approximately be distributed in same way the training samples. \n","\n","Indeed, if we have $\\mathbf{z}^i \\sim \\mathcal{N}(0,1)$ then $S^{-1}(\\mathbf{z}^i)\\sim \\pi(\\mathbf{x})$:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TVUMpRr8T-Tg"},"outputs":[],"source":["#Draw new samples from the learned distribution:\n","Znew = np.random.randn(2,5000)\n","\n","colors = np.arctan2(Znew[1,:],Znew[0,:])\n","Xnew = S.Inverse(Znew,Znew)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FozEaXNzTu4B"},"outputs":[],"source":["#Plot\n","fig,ax = plt.subplots(1,2)\n","ax[0].scatter(Znew[0,:],Znew[1,:], c=colors, alpha=0.2, label='Normal samples')\n","ax[0].set_title('Normal samples')\n","ax[0].set_aspect('equal', 'box')\n","ax[0].set_xlim((-4.5, 4.5))\n","ax[0].set_ylim((-4.5, 4.5))\n","ax[1].scatter(Xnew[0,:],Xnew[1,:], c=colors, alpha=0.2, label='New samples')\n","ax[1].set_title('New samples')\n","ax[1].set_aspect('equal', 'box')\n","ax[1].set_xlim((-1.75, 1.25))\n","ax[1].set_ylim((-1, 2))\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"291pJ_9YWllE"},"source":["#### Density estimation:"]},{"cell_type":"markdown","metadata":{"id":"XN9FpASduPvL"},"source":["Another use of the transport map $S$ is to estimate the probability function of $\\pi(\\mathbf{x})$. Indeed, in that case $\\pi(\\mathbf{x}) \\approx [S^\\sharp \\rho](\\mathbf{x})$."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UUzBZR_AuPXy"},"outputs":[],"source":["def pullback_pdf(tri_map,rho,x):\n","  y = tri_map.Evaluate(x)\n","  log_pdf = rho.logpdf(y.T)+tri_map.LogDeterminant(x)\n","  return np.exp(log_pdf)\n","\n","Ngrid = 100\n","x = np.linspace(-1.5, 1., Ngrid)\n","y = np.linspace(-1, 1.5, Ngrid)\n","X, Y = np.meshgrid(x, y)\n","XY = np.vstack((X.flatten(),Y.flatten()))\n","\n","rho = multivariate_normal(np.zeros(2),np.eye(2))\n","\n","Z = pullback_pdf(S,rho,XY)\n","Z = Z.reshape(Ngrid,Ngrid)\n","\n","fig, ax = plt.subplots()\n","CS = ax.contour(X, Y, Z)\n","ax.set_aspect('equal', 'box')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"RDTPuoY2UA-j"},"source":["## Characterization of Bayesian inference posteriors via transport maps"]},{"cell_type":"markdown","metadata":{"id":"nM3TDiTyXiS9"},"source":["### Bayesian inference"]},{"cell_type":"markdown","metadata":{"id":"61mDS9c_XESi"},"source":["One other way to build transport maps is from unnormlized density. One situation where we know the probality density function up to a normalization constant is when modeling inverse problems with Bayesian inference."]},{"cell_type":"markdown","metadata":{"id":"05Xh-04NYSJp"},"source":["For an inverse problem, the objective is to characterize the value of some parameters $\\boldsymbol{\\theta}$ of a given system, knowing some the value of some noisy observations $\\mathbf{y}$. "]},{"cell_type":"markdown","metadata":{"id":"hIA-PjEgY8bT"},"source":["With Bayesian inference, the characterization of parameters $\\boldsymbol{\\theta}$ is done via a *posterior* density $\\pi(\\boldsymbol{\\theta}|\\mathbf{y})$. This density characterize the distribution of the parameters knowing the value of the observations."]},{"cell_type":"markdown","metadata":{"id":"_yLFY5fmZi1t"},"source":["The posterior can thus be decomposed by the product of two probabilistic quantities:\n","\n","\n","1.   The prior density $\\pi(\\boldsymbol{\\theta})$ which is use to enforce any *a priori* knowledge about the parameters.\n","2.   The likelihood function $\\pi(\\mathbf{y}|\\boldsymbol{\\theta})$. This quantity seen as a function of $\\boldsymbol{\\theta}$ gives the proability that the considered system produce the observation $\\mathbf{y}$ for a fixed value of $\\boldsymbol{\\theta}$. When the model that describes the system is known in closed form, the likelihood function is also knwon in closed form.\n","\n","Hence, the posterior density reads:\n","\\begin{equation}\n","\\pi(\\boldsymbol{\\theta}|\\mathbf{y}) = \\frac{1}{c} \\pi(\\mathbf{y}|\\boldsymbol{\\theta}).\\pi(\\boldsymbol{\\theta})\n","\\end{equation}\n","where $c$ is an unknown real constant that ensure that the product of the two quantities is a proper density.\n"]},{"cell_type":"markdown","metadata":{"id":"MOmf-AOZcTIu"},"source":["### Problem description"]},{"cell_type":"markdown","metadata":{"id":"uqDP_VK_chGe"},"source":["Knowing the closed form of unnormalized posterior $\\bar{\\pi}(\\boldsymbol{\\theta} |\\mathbf{y})= \\pi(\\mathbf{y}|\\boldsymbol{\\theta}).\\pi(\\boldsymbol{\\theta})$, the objective is to draw samples from/approximate the posterior density.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"INBIqSDpdf0F"},"source":["### Map from density"]},{"cell_type":"markdown","metadata":{"id":"B5WNmHEmddwz"},"source":["In order to characterize this posterior density, one method is to build a transport map."]},{"cell_type":"markdown","metadata":{"id":"8x05jV8yd6bk"},"source":["For the map from unnormalized density estimation, the objective function reads:"]},{"cell_type":"markdown","metadata":{"id":"7xfNYRnneGet"},"source":["\\begin{equation}\n","J(\\mathbf{w}) = - \\frac{1}{N}\\sum_{i=1}^N \\left( \\log\\pi\\left(T(\\mathbf{z}^i;\\mathbf{w})\\right) + \\log  \\text{det }\\nabla_\\mathbf{z} T(\\mathbf{z}^i;\\mathbf{w})\\right), \\,\\,\\, \\mathbf{z}^i \\sim \\mathcal{N}(\\mathbf{0},\\mathbf{I}_d)\n","\\end{equation}\n","where $T$ is the transport map pushing forward the standard normal $mathcal{N}(\\mathbf{0},\\mathbf{I}_d)$ to the target density $\\pi(\\mathbf{x})$ (here the target density will be the posterior density)."]},{"cell_type":"markdown","metadata":{"id":"kduvKQ6dePKz"},"source":["Gradient of the objective function in that case reads:"]},{"cell_type":"markdown","metadata":{"id":"lle8G9j7eN6n"},"source":["\\begin{equation}\n","\\nabla_\\mathbf{w} J(\\mathbf{w}) = - \\frac{1}{N}\\sum_{i=1}^N \\left( \\nabla_\\mathbf{w} T(\\mathbf{z}^i;\\mathbf{w}).\\nabla_\\mathbf{x}\\log\\pi\\left(T(\\mathbf{z}^i;\\mathbf{w})\\right) + \\nabla_{\\mathbf{w}}\\log  \\text{det }\\nabla_\\mathbf{z} T(\\mathbf{z}^i;\\mathbf{w})\\right), \\,\\,\\, \\mathbf{z}^i \\sim \\mathcal{N}(\\mathbf{0},\\mathbf{I}_d)\n","\\end{equation}"]},{"cell_type":"markdown","metadata":{"id":"z4NQ5TfSfHbC"},"source":["### Example: Biochemical Oxygen Demand (BOD) model\n","(example taken from https://arxiv.org/pdf/1602.05023.pdf)"]},{"cell_type":"markdown","metadata":{"id":"Z0bDu_mKfn0X"},"source":["#### Model:\n","The time dependent forward model is defined as:\n","\\begin{equation}\n","\\mathcal{B}(t) = A(1-\\exp(-Bt))+\\mathcal{E}\n","\\end{equation}\n","where:\n","\\begin{equation}\n","\\mathcal{E} \\sim \\mathcal{N}(0,1e-3)\n","\\end{equation}\n","\\begin{equation}\n","A = \\left[0.4 + 0.4\\left(1 + \\text{erf}\\left(\\frac{\\theta_1}{\\sqrt{2}} \\right)\\right) \\right]\n","\\end{equation}\n","\\begin{equation}\n","B = \\left[0.01 + 0.15\\left(1 + \\text{erf}\\left(\\frac{\\theta_2}{\\sqrt{2}} \\right)\\right) \\right]\n","\\end{equation}"]},{"cell_type":"markdown","metadata":{"id":"gvUHx0CcheOn"},"source":["The objective is to characterize the posterior density of parameters $\\boldsymbol{\\theta}=(\\theta_1,\\theta_2)$ knowing observation of the system at time $t=\\left\\{1,2,3,4,5 \\right\\}$ i.e. $\\mathbf{y} = (\\mathcal{B}(1),\\mathcal{B}(2),\\mathcal{B}(3),\\mathcal{B}(4),\\mathcal{B}(5))$."]},{"cell_type":"markdown","metadata":{"id":"vfE_x8ItUQpl"},"source":["#### Deterministic model:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FnTyhgc0T8lf"},"outputs":[],"source":["def forward_model(p1,p2,t):\n","  A = 0.4+0.4*(1+scipy.special.erf(p1/np.sqrt(2)))\n","  B = 0.01+0.15*(1+scipy.special.erf(p2/np.sqrt(2)))\n","  out = A*(1-np.exp(-B*t))\n","  return out\n","\n","def grad_x_forward_model(p1,p2,t):\n","  A = 0.4+0.4*(1+scipy.special.erf(p1/np.sqrt(2)))\n","  B = 0.01+0.15*(1+scipy.special.erf(p2/np.sqrt(2)))\n","  dAdx1 = 0.31954*np.exp(-0.5*p1**2)\n","  dBdx2 = 0.119683*np.exp(-0.5*p2**2)\n","  dOutdx1 = dAdx1*(1-np.exp(-B*t))\n","  dOutdx2 = t*A*dBdx2*np.exp(-t*B)\n","  return np.vstack((dOutdx1,dOutdx2))"]},{"cell_type":"markdown","metadata":{"id":"MXj4SeMvUfsj"},"source":["#### Likelihood functions:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B2PSr1BnUaun"},"outputs":[],"source":["def log_likelihood(std_noise,t,yobs,p1,p2):\n","  y = forward_model(p1,p2,t)\n","  log_lkl = -np.log(np.sqrt(2*np.pi)*std_noise)-0.5*((y-yobs)/std_noise)**2\n","  return log_lkl\n","\n","def grad_x_log_likelihood(std_noise,t,yobs,p1,p2):\n","  y = forward_model(p1,p2,t)\n","  dydx = grad_x_forward_model(p1,p2,t)\n","  grad_x_lkl = (-1/std_noise**2)*(y - yobs)*dydx\n","  return grad_x_lkl"]},{"cell_type":"markdown","metadata":{"id":"vbqP5DiHUo2G"},"source":["#### Unnormalized posterior density:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W5VhvAweUi6h"},"outputs":[],"source":["def log_posterior(std_noise,std_prior1,std_prior2,list_t,list_yobs,p1,p2):\n","  log_prior1 = -np.log(np.sqrt(2*np.pi)*std_prior1)-0.5*(p1/std_prior1)**2\n","  log_prior2 = -np.log(np.sqrt(2*np.pi)*std_prior2)-0.5*(p2/std_prior2)**2\n","  log_posterior = log_prior1+log_prior2\n","  for k,t in enumerate(list_t):\n","    log_posterior += log_likelihood(std_noise,t,list_yobs[k],p1,p2)\n","  return log_posterior\n","\n","def grad_x_log_posterior(std_noise,std_prior1,std_prior2,list_t,list_yobs,p1,p2):\n","  grad_x1_prior = -(1/std_prior1**2)*(p1)\n","  grad_x2_prior = -(1/std_prior2**2)*(p2)\n","  grad_x_prior = np.vstack((grad_x1_prior,grad_x2_prior))\n","  grad_x_log_posterior = grad_x_prior \n","  for k,t in enumerate(list_t):\n","    grad_x_log_posterior += grad_x_log_likelihood(std_noise,t,list_yobs[k],p1,p2)\n","  return grad_x_log_posterior"]},{"cell_type":"markdown","metadata":{"id":"M2c_CaNNU23t"},"source":["#### Example with 5 observations"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QB909TMQUryu"},"outputs":[],"source":["list_t = np.array([1,2,3,4,5])\n","list_yobs = np.array([0.18,0.32,0.42,0.49,0.54])\n","\n","std_noise = np.sqrt(1e-3);\n","std_prior1 = 1;\n","std_prior2 = 1;"]},{"cell_type":"markdown","metadata":{"id":"qCRL8_P1Wr8h"},"source":["Visualization of the **unnormalized** posterior density:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aMRLess1bhK6"},"outputs":[],"source":["Ngrid = 100\n","x = np.linspace(-0.5, 2.5, Ngrid)\n","y = np.linspace(-0.5, 2.5, Ngrid)\n","X, Y = np.meshgrid(x, y)\n","\n","Z = log_posterior(std_noise,std_prior1,std_prior2,list_t,list_yobs,X.flatten(),Y.flatten())\n","Z = np.exp(Z.reshape(Ngrid,Ngrid))\n","\n","\n","fig, ax = plt.subplots()\n","CS = ax.contour(X, Y, Z)\n","ax.set_xlabel(r'$\\theta_1$')\n","ax.set_ylabel(r'$\\theta_2$')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"guYpL1CNXTkm"},"source":["#### Definition of the objective function:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mVDIKKPHXSu0"},"outputs":[],"source":["def grad_x_log_target(x):\n","  out = grad_x_log_posterior(std_noise,std_prior1,std_prior2,list_t,list_yobs,x[0,:],x[1,:])\n","  return out\n","\n","def log_target(x):\n","  out = log_posterior(std_noise,std_prior1,std_prior2,list_t,list_yobs,x[0,:],x[1,:])\n","  return out\n","\n","def obj(coeffs, tri_map,x):\n","    num_points = x.shape[1]\n","    tri_map.SetCoeffs(coeffs)\n","    map_of_x = tri_map.Evaluate(x)\n","    rho_of_map_of_x = log_target(map_of_x)\n","    log_det = tri_map.LogDeterminant(x)\n","    return -np.sum(rho_of_map_of_x + log_det)/num_points\n","\n","def grad_obj(coeffs, tri_map, x):\n","    num_points = x.shape[1]\n","    tri_map.SetCoeffs(coeffs)\n","    map_of_x = tri_map.Evaluate(x)\n","    sensi = grad_x_log_target(map_of_x)\n","    grad_rho_of_map_of_x = tri_map.CoeffGrad(x, sensi) \n","    grad_log_det = tri_map.LogDeterminantCoeffGrad(x)\n","    return -np.sum(grad_rho_of_map_of_x + grad_log_det, 1)/num_points"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JKOzmtbEXfqB"},"outputs":[],"source":["#Draw reference samples\n","N=10000\n","Xtrain = np.random.randn(2,N)"]},{"cell_type":"markdown","metadata":{"id":"DHkXWX0NXm1_"},"source":["#### Map parametrization and optimization:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R-ER6wtwXhUc"},"outputs":[],"source":["# Set-up map and initize map coefficients\n","opts = MapOptions()\n","\n","map_order = 3\n","tri_map = CreateTriangular(2,2,map_order,opts)\n","coeffs = np.zeros(tri_map.numCoeffs)\n","tri_map.SetCoeffs(coeffs)\n","\n","options={'gtol': 1e-2, 'disp': True}\n","res = minimize(obj, tri_map.CoeffMap(), args=(tri_map, Xtrain), jac=grad_obj, method='BFGS', options=options)\n"]},{"cell_type":"markdown","metadata":{"id":"Ju10kojxjc9w"},"source":["#### Comparing contours of approximate posterior and true unnormalized posterior"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Eu0zwM89Z8fG"},"outputs":[],"source":["def push_forward_pdf(tri_map,rho,x):\n","  xinv = tri_map.Inverse(x,x)\n","  log_det_grad_x_inverse = - tri_map.LogDeterminant(xinv)\n","  log_pdf = rho.logpdf(xinv.T)+log_det_grad_x_inverse\n","  return np.exp(log_pdf)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F-5wPAGDYPv7"},"outputs":[],"source":["Ngrid = 100\n","x = np.linspace(-0.5, 1.3, Ngrid)\n","y = np.linspace(-0.2, 2.3, Ngrid)\n","xx, yy = np.meshgrid(x, y)\n","\n","xx_eval = np.vstack((xx.flatten(),yy.flatten()))\n","Z2 = push_forward_pdf(tri_map,rho,xx_eval)\n","Z2 = Z2.reshape(Ngrid,Ngrid)\n","\n","Z = log_posterior(std_noise,std_prior1,std_prior2,list_t,list_yobs,xx.flatten(),yy.flatten())\n","Z = np.exp(Z.reshape(Ngrid,Ngrid))\n","\n","fig, ax = plt.subplots()\n","CS1 = ax.contour(xx, yy, Z)\n","CS2 = ax.contour(xx, yy, Z2,linestyles='dashed')\n","ax.set_xlabel(r'$\\theta_1$')\n","ax.set_ylabel(r'$\\theta_2$')\n","h1,_ = CS1.legend_elements()\n","h2,_ = CS2.legend_elements()\n","ax.legend([h1[0], h2[0]], ['Unnormilzed posterior', 'TM approximation'])\n","plt.show()\n","np.savetxt('Zpost.txt',Z)"]},{"cell_type":"markdown","metadata":{"id":"tO12yfcI-8SQ"},"source":["### Drawing samples from approximate posterior\n","\n","Once the transport map from reference to unnormalized posterior is estimated it can be used to sample from the posterior to characterize the Bayesian inference solution. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZqqSOnUN-_MG"},"outputs":[],"source":["Znew = np.random.randn(2,15000)\n","colors = np.arctan2(Znew[1,:],Znew[0,:])\n","\n","Xpost = tri_map.Evaluate(Znew)\n","fig,ax = plt.subplots()\n","ax.scatter(Xpost[0,:],Xpost[1,:], c=colors, alpha=0.2, label='Posterior samples')\n","ax.set_aspect('equal', 'box')\n","ax.set_xlabel(r'$\\theta_1$')\n","ax.set_xlabel(r'$\\theta_2$')\n","ax.legend()\n","plt.show()\n","\n","np.savetxt('Xpost.txt',Xpost)"]},{"cell_type":"markdown","metadata":{"id":"cpeZF5TjCYOX"},"source":["Mean a posteriori:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VZDvAgtUCcPU"},"outputs":[],"source":["X_mean = np.mean(Xpost,1)\n","print('Mean a posteriori: '+str(X_mean))"]},{"cell_type":"markdown","metadata":{"id":"kvedTHn0j2SX"},"source":["One-dimensional marginals histograms:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n-kH0LBqB8d4"},"outputs":[],"source":["fig, ax = plt.subplots(1,2)\n","ax[0].hist(Xpost[0,:], 50, alpha=0.5, density=True)\n","ax[0].set_xlabel(r'$\\theta_1$')\n","ax[1].hist(Xpost[1,:], 50, alpha=0.5, density=True)\n","ax[1].set_xlabel(r'$\\theta_2$')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"TBiFp-wRKBPu"},"source":["## Posterior density from samples (Likelihood Free Inference)"]},{"cell_type":"markdown","metadata":{"id":"8nPVPr-TfT3B"},"source":["One other way to characterize the Bayesian inference solution is to use the property of triangular maps that allows characterization of conditional densities."]},{"cell_type":"markdown","metadata":{"id":"o77hvdmCfyCy"},"source":["Indeed, let's consider two random vectors $\\mathbf{x}$ and $\\mathbf{y}$. If the triangular map $S$ with structure:\n","\\begin{equation}\n","S=\n","\\begin{bmatrix}\n","S_y(\\mathbf{y})\\\\\n","S_x(\\mathbf{y},\\mathbf{x})\n","\\end{bmatrix}\n","\\end{equation}\n","is defined as:\n","\\begin{equation}\n","\\left[S^\\sharp \\eta\\right](\\mathbf{y},\\mathbf{x}) = \\pi(\\mathbf{y},\\mathbf{x}),\n","\\end{equation}\n","then the conditional density $\\pi(\\mathbf{x}|\\mathbf{y})$ can be defined with the lower block of $S$ as:\n","\\begin{equation}\n","S_x(\\mathbf{y},.)^\\sharp \\eta(\\mathbf{x}) = \\pi(\\mathbf{x}|\\mathbf{y})\n","\\end{equation}"]},{"cell_type":"markdown","metadata":{"id":"nI9TQDhkiaeJ"},"source":["Applied to posterior characterization, this leads to computing the transport map between the joint density of observation and parameters $\\pi(\\mathbf{y},\\boldsymbol{\\theta})$ and reference density $\\eta$."]},{"cell_type":"markdown","metadata":{"id":"HW2pUi-0lhzp"},"source":["### Generation of joint samples $\\{\\mathbf{y}^i,\\boldsymbol{\\theta}^i \\}$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Pwpr9YcU2KN"},"outputs":[],"source":["N = 10000\n","X = np.random.randn(2,N)\n","\n","Y = np.zeros((5,N))\n","\n","for t in range(5):\n","  Y[t,:]=forward_model(X[0,:],X[1,:],t+1)+std_noise*np.random.randn(1,N)\n","\n","YX = np.vstack((Y,X))"]},{"cell_type":"markdown","metadata":{"id":"hIM38yyMl1E0"},"source":["### Normalization of samples\n","\n","It is often beneficial to normalized training samples before computing a transport map. In order to that we simply define a linear map defined by mean and coviarance of training samples. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IrG6u8nilzSI"},"outputs":[],"source":["mu = np.mean(YX,1)\n","std_data = np.std(YX,1)\n","\n","Linv = np.diag(1/std_data)\n","mu_inv = - np.dot(Linv,mu)\n","\n","YXnorm = mu_inv.reshape(-1,1)+np.dot(Linv,YX)"]},{"cell_type":"markdown","metadata":{"id":"fclpzh3MmO5E"},"source":["### Finding lower block map components"]},{"cell_type":"markdown","metadata":{"id":"dA3TtR8Gmfaz"},"source":["To characterize the posterior density from the map that characterizes the joint density, we actually only need to compute the map components corresponding to parameters. Hence in our Bayesian example, from the joint map of dimension 7 (5 observations + 2 parameters), we only need to compute the last two compoenents. Then, we can use the property of decomposability of the minimization problems to separately find map components 6 and 7."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C61zK_i8vWGI"},"outputs":[],"source":["# Negative log likelihood objective\n","\n","rho1 = multivariate_normal(np.zeros(1),np.eye(1))\n","def obj(coeffs, tri_map,x):\n","    num_points = x.shape[1]\n","    tri_map.SetCoeffs(coeffs)\n","    map_of_x = tri_map.Evaluate(x)\n","    rho_of_map_of_x = rho1.logpdf(map_of_x.T)\n","    log_det = tri_map.LogDeterminant(x)\n","    return -np.sum(rho_of_map_of_x + log_det)/num_points\n","\n","def grad_obj(coeffs, tri_map, x):\n","    num_points = x.shape[1]\n","    tri_map.SetCoeffs(coeffs)\n","    map_of_x = tri_map.Evaluate(x)\n","    grad_rho_of_map_of_x = -tri_map.CoeffGrad(x, map_of_x)\n","    grad_log_det = tri_map.LogDeterminantCoeffGrad(x)\n","    return -np.sum(grad_rho_of_map_of_x + grad_log_det, 1)/num_points"]},{"cell_type":"markdown","metadata":{"id":"mPTsbfNGn611"},"source":["#### Component 6:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hwJEMipk6Wku"},"outputs":[],"source":["#multis6 = np.loadtxt('/content/multis6.txt',delimiter=',')\n","#mset6 = MultiIndexSet(multis6)\n","#fixed_mest6 = mset6.fix(True)\n","\n","fixed_mset6 = FixedMultiIndexSet(6,4)\n","\n","S6 = CreateComponent(fixed_mset6,opts)\n","Xtrain = YXnorm[:6,:]\n","\n","options={'gtol': 1e-2, 'disp': True}\n","res2 = minimize(obj, S6.CoeffMap(), args=(S6, Xtrain), jac=grad_obj, method='BFGS', options=options)"]},{"cell_type":"markdown","metadata":{"id":"4bnE9gZooB2y"},"source":["#### Component 7:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"S4_Pxj5YKeaa"},"outputs":[],"source":["#multis7 = np.loadtxt('/content/multis7.txt',delimiter=',')\n","#mset7 = MultiIndexSet(multis7)\n","#fixed_mest7 = mset7.fix(True)\n","\n","fixed_mset7 = FixedMultiIndexSet(7,3)\n","\n","S7 = CreateComponent(fixed_mset7,opts)\n","\n","Xtrain = YXnorm\n","\n","options={'gtol': 1e-2, 'disp': True}\n","res2 = minimize(obj, S7.CoeffMap(), args=(S7, Xtrain), jac=grad_obj, method='BFGS', options=options)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jHO5rmLC1tDk"},"outputs":[],"source":["cond_map = TriangularMap((S6,S7))\n","cond_map.SetCoeffs(np.concatenate((S6.CoeffMap(),S7.CoeffMap())))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hkMDUgrxzQwa"},"outputs":[],"source":["def cond_composed_pullback_pdf(tri_map,mu,L,rho,y,x):\n","  y = y.reshape(-1,1)*np.ones((len(y),x.shape[1]))\n","  yx = np.vstack((y,x))\n","  Lyx = mu.reshape(-1,1)+np.dot(L,yx)\n","  eval = tri_map.Evaluate(Lyx)\n","  log_pdf = rho.logpdf(eval.T)+tri_map.LogDeterminant(yx)+np.log(np.linalg.det(Linv))\n","  return np.exp(log_pdf)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yl5WXoa5DGQE"},"outputs":[],"source":["Ngrid = 100\n","x = np.linspace(-0.7, 2.5, Ngrid)\n","y = np.linspace(-0.7, 2.5, Ngrid)\n","xx, yy = np.meshgrid(x, y)\n","\n","xx_eval = np.vstack((xx.flatten(),yy.flatten()))\n","Z2 = cond_composed_pullback_pdf(cond_map,mu_inv,Linv,rho,list_yobs,xx_eval)\n","Z2 = Z2.reshape(Ngrid,Ngrid)\n","\n","Z = log_posterior(std_noise,std_prior1,std_prior2,list_t,list_yobs,xx.flatten(),yy.flatten())\n","Z = np.exp(Z.reshape(Ngrid,Ngrid))\n","\n","fig, ax = plt.subplots()\n","CS1 = ax.contour(xx, yy, Z)\n","CS2 = ax.contour(xx, yy, Z2,linestyles='dashed')\n","h1,_ = CS1.legend_elements()\n","h2,_ = CS2.legend_elements()\n","ax.legend([h1[0], h2[0]], ['Unnormilzed posterior', 'TM approximation'])\n","ax.set_xlabel(r'$\\theta_1$')\n","ax.set_ylabel(r'$\\theta_2$')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"I6bniYDvoJgl"},"source":["Advantages of this method is that we don't need to explicity know the model, the likelihood function its derivates etc. The likelihood function is implicitly learned directly from samples. One drawback though is that map components are higher dimensional than in the map from density case."]}],"metadata":{"colab":{"collapsed_sections":[],"name":"tutorial_code_AN22.ipynb","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
